# ‚öóÔ∏è Knowledge Distillation: Teacher-Student Networks

**Facilitators:** Adam Kosiorek / Ruba Haroun (Google DeepMind), Andres Villa (KAUST)  
**Track:** Advanced  

## üìñ Session Overview
This hands-on workshop dives into **Knowledge Distillation (KD)**, a powerful technique for model compression and performance enhancement. You will learn how to transfer the "dark knowledge" from a large, pre-trained **Teacher** model (ResNet-18) to a smaller, more efficient **Student** model.

The session covers the theory behind soft labels and temperature scaling, and guides you through implementing and training a complete teacher-student framework from scratch.

## ‚ö†Ô∏è Requirements
This notebook is designed to run on a **GPU Runtime** for efficient training.
