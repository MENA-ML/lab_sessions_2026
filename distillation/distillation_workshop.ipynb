{"cells":[{"cell_type":"markdown","source":["# ðŸŽ“ 2026 MenaML: Distillation Workshop\n","- Connect to the T4 GPU!\n"],"metadata":{"id":"2UGmCtCn5Eg_"}},{"cell_type":"markdown","metadata":{"id":"9sV45b9CU2Kh"},"source":["## 1. Introduction: What is Knowledge Distillation?\n","\n","**Knowledge Distillation** is a technique whereby a student neural network learns from another, usually already pre-trained neural network. KD can be used for compressing the model, in which case the student is smaller. It can also be used for improving model performance, where the student is the same or even larger than the teacher. In either case, the the student is trained to mimic the behavior of one or more teacher models.\n","\n","### Why do we need it?\n","\n","- **Deployment constraints**: Large models are expensive to run on edge devices, mobile phones, or in real-time applications\n","- **Inference speed**: Smaller models are faster\n","- **Cost reduction**: Less compute = less money and energy\n","- **Increased performance**: Sometimes KD is used to increase the performance of our model."]},{"cell_type":"markdown","metadata":{"id":"-h9dET-pU2Kh"},"source":["\n","\n","\n","## Learning Objectives\n","\n","By the end of this workshop, you will:\n","1. Understand the core concepts behind Distillation\n","2. Implement a Teacher-Student training framework from scratch\n","3. Compare different training strategies: baseline, hard labels, soft labels\n","4. Analyze the trade-offs between model size and performance\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"mmk4A2LunBrH"},"source":["## Why Does KD work?\n","\n","### The \"Dark Knowledge\"\n","\n","In their seminal 2015 paper, [Hinton et al](https://arxiv.org/abs/1503.02531). observed that the **soft probability outputs** of a teacher model contain more information than hard labels.\n","\n","**Example**: For a cat image, hard label says `[0, 0, 1, 0, ...]` (just \"cat\")  \n","But soft labels might say `[0.01, 0.05, 0.85, 0.09, ...]` revealing that the image also looks a bit like a dog or tiger!\n","\n","This extra information about class relationships is the \"dark knowledge\" that helps the student learn better.\n","\n","![Distillation Diagram](https://intellabs.github.io/distiller/imgs/knowledge_distillation.png)\n","\n","\n","### Reweighing Training Examples\n","\n","Even if we ignore probabilities for all classes other than the true class, we get an effect where some examples are weighted higher than other ones according to what probability the teacher assigns to the true class. [Tang et al](https://arxiv.org/abs/2002.03532) showed that this *importance weighing* is an important component of how KD works."]},{"cell_type":"markdown","metadata":{"id":"n6FpaMfYU2Ki"},"source":["## 2. Setup and Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"akmnW8gjU2Ki"},"outputs":[],"source":["import collections\n","import os\n","import time\n","from typing import Dict, List, Optional, Tuple, Union\n","\n","# Third party\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision import models\n","import tqdm.auto as tqdm\n","import random\n","\n","# Check GPU availability\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","if torch.cuda.is_available():\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"]},{"cell_type":"code","source":["def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","set_seed(10)"],"metadata":{"id":"gbjcIkY6ruqv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nRWjNHReU2Ki"},"source":["## 3. Dataset: CIFAR-10\n","\n","We'll use CIFAR-10: 60,000 32x32 color images in 10 classes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s1MXgRc-U2Ki"},"outputs":[],"source":["# Data augmentation and normalization\n","cifar_mean = np.asarray([0.4914, 0.4822, 0.4465])\n","cifar_std = np.asarray([0.2023, 0.1994, 0.2010])\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=cifar_mean, std=cifar_std),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=cifar_mean, std=cifar_std),\n","])\n","\n","# Download and load datasets\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","\n","trainloader = DataLoader(\n","    trainset, batch_size=128, shuffle=True, num_workers=0,\n",")\n","testloader = DataLoader(\n","    testset, batch_size=128, shuffle=False, num_workers=0,\n",")\n","\n","classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","print(f\"Training samples: {len(trainset)}\")\n","print(f\"Test samples: {len(testset)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9oySRHEcU2Ki"},"outputs":[],"source":["# Visualize some samples\n","def imshow(ax, img: torch.Tensor) -> None:\n","    \"\"\"Helper function to un-normalize and display an image.\n","\n","    Args:\n","        ax: Matplotlib axes to plot on.\n","        img (torch.Tensor): Tensor image of shape (C, H, W).\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    img = img * cifar_std[:, None, None] + cifar_mean[:, None, None]\n","    img = np.clip(img, 0., 1.)\n","    npimg = img.numpy()\n","    ax.imshow(np.transpose(npimg, (1, 2, 0)))\n","\n","# Get some random training images\n","dataiter = iter(trainloader)\n","images, labels = next(dataiter)\n","\n","# Show images\n","fig, axes = plt.subplots(1, 8, figsize=(6, 1))\n","for i in range(8):\n","    # axes[i].imshow(np.transpose((images[i] / 2 + 0.5).numpy(), (1, 2, 0)))\n","    imshow(axes[i], images[i])\n","    axes[i].set_title(classes[labels[i]])\n","    axes[i].axis('off')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Buk8aA4EU2Ki"},"source":["## 4. Our Models: Convolutional Neural Networks (CNNs)\n","\n","Convolution is a linear operation where you multiply pixels from a neighbourhood with a filter and sum the results. This way you get a number for the center of the filter. You apply the convolution to an image by sliding the filter along the image. This is ilustrated in this animation.\n","![convolution](https://d29g4g2dyqv443.cloudfront.net/sites/default/files/pictures/2018/convolution-2.gif)\n","\n","A convolutional layer applies many such filter to an image in parallel. You can build a convolutional neural network by stacking such layers one after the other, like in the image below.\n","\n","![CNN](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/10/90650dnn2.webp)\n","\n","The models we use here are CNNs. The teacher model is large, with 11M parameters, and it is a standard architecture called ResNet-18. A ResNet is similar to a CNN, but it adds skip-connections between layers.\n","\n","![ResNet](https://cdn.prod.website-files.com/680a070c3b99253410dd3df5/684d85026cf20a8dfe9fa8fd_6835cb463baf4440d0a5d1c5_Resnet50_fig1.webp)\n","\n","Our student is a small custom CNN with under 1M parameters.\n","\n","The goal is to transfer knowledge from the large teacher to the tiny student."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vYja44irU2Ki"},"outputs":[],"source":["class TeacherCNN(nn.Module):\n","    \"\"\"Teacher: ResNet-18 adapted for CIFAR-10 (32x32 images).\n","\n","    Args:\n","      num_classes: the number of outputs in the last layer. For Cifar-10 we use\n","                   10 outputs.\n","      weights: The name of the weights. With None, the weights will be randomly\n","               initialized. You can try 'IMAGENET1K_V1' to load weights trained\n","               on the ImageNet1K dataset. These should be much stronger than our\n","               provided weights.\n","    \"\"\"\n","    def __init__(self, num_classes: int = 10, weights: Optional[str] = None):\n","        super(TeacherCNN, self).__init__()\n","        self.model = models.resnet18(weights=weights)\n","        # Modify first conv layer for 32x32 images (no aggressive downsampling)\n","        self.model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.model.maxpool = nn.Identity()  # Remove maxpool for small images\n","        self.fc = nn.Linear(self.model.fc.in_features, num_classes)\n","        self.model.fc = nn.Identity()\n","\n","    def forward(self, x, output_features=False):\n","        x = self.model(x)\n","        logits = self.fc(x)\n","        if output_features:\n","            return logits, x\n","        return logits\n","\n","\n","class StudentCNN(nn.Module):\n","    \"\"\"\n","    Student: A small CNN with ~100K parameters\n","    \"\"\"\n","    def __init__(self, num_classes: int = 10, output_dim: int = 256):\n","        super(StudentCNN, self).__init__()\n","        self.features = nn.Sequential(\n","            # Block 1\n","            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),  # 32 -> 16\n","\n","            # Block 2\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),  # 16 -> 8\n","\n","            # Block 3\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),  # 8 -> 4\n","\n","            # Block 4\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),  # 4 -> 2\n","\n","            # Block 5\n","            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),  # 2 -> 1\n","        )\n","\n","        self.fc1 = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(512 * 1 * 1, output_dim),  # 512 to match teacher\n","            nn.ReLU(),\n","        )\n","        self.fc2 = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(output_dim, num_classes),\n","        )\n","\n","    def forward(self, x, output_features=False):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc1(x)\n","        logits = self.fc2(x)\n","        if output_features:\n","            return logits, x\n","        return logits\n","\n","\n","def count_parameters(model):\n","    \"\"\"Count trainable parameters\"\"\"\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"]},{"cell_type":"markdown","metadata":{"id":"WctECz5RJ5MS"},"source":["### Activity 1. Instantiate the Teacher and Student models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J6iucLqgLeRX"},"outputs":[],"source":["# TODO\n","teacher = None\n","student = None"]},{"cell_type":"markdown","metadata":{"id":"zWsLk8bmJ1wL"},"source":["#### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bh55SIbzU2Ki","cellView":"form"},"outputs":[],"source":["# @title\n","# Create models and compare sizes\n","teacher = TeacherCNN().to(device)\n","student = StudentCNN().to(device)"]},{"cell_type":"markdown","metadata":{"id":"ewVy-h6dnBrI"},"source":["### Compare Student and Teacher Model Sizes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLn0g_JknBrI"},"outputs":[],"source":["teacher_params = count_parameters(teacher)\n","student_params = count_parameters(student)\n","\n","print(f\"Teacher (ResNet-18) parameters: {teacher_params:,}\")\n","print(f\"Student (Small CNN) parameters: {student_params:,}\")\n","print(f\"\\nCompression ratio: {teacher_params / student_params:.1f}x smaller\")"]},{"cell_type":"markdown","metadata":{"id":"MUd59_i3U2Kj"},"source":["## 5. Training Utilities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gu1GAzEyU2Kj"},"outputs":[],"source":["def train_epoch(model, teacher, trainloader, criterion, optimizer, device):\n","    \"\"\"Trains the model using supervised training or distillation.\n","\n","    Distillation is used only when the teacher is present. In that case, the\n","    teacher is frozen (no gradients).\n","    \"\"\"\n","    model.train()\n","    if teacher is not None:\n","      teacher.eval()  # Teacher is always in eval mode\n","\n","    running_metrics = collections.defaultdict(int)\n","    correct = 0\n","    total = 0\n","\n","    for inputs, labels in tqdm.tqdm(trainloader, desc=\"Training\", leave=False):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # Forward pass for the trained model\n","        optimizer.zero_grad()\n","        logits = model(inputs)\n","\n","        # Compute loss\n","        if teacher is not None:\n","            # Get teacher predictions (no gradient needed)\n","            with torch.no_grad():\n","                teacher_logits = teacher(inputs)\n","            loss, metrics = criterion(\n","                logits, teacher_logits, labels\n","            )\n","        else:\n","          loss = criterion(logits, labels)\n","          metrics = {}\n","\n","        # Backward and optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Metrics\n","        running_metrics['loss'] += loss.item()\n","        for k, v in metrics.items():\n","            running_metrics[k] += v\n","        _, predicted = logits.max(1)\n","        total += labels.size(0)\n","        correct += predicted.eq(labels).sum().item()\n","\n","    n = len(trainloader)\n","    results = {k: v/n for k, v in running_metrics.items()}\n","    results['train_acc'] = 100. * correct / total\n","    return results\n","\n","\n","def evaluate(model, testloader, device):\n","    \"\"\"Evaluate model on test set\"\"\"\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in testloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = outputs.max(1)\n","            total += labels.size(0)\n","            correct += predicted.eq(labels).sum().item()\n","\n","    return dict(test_acc=100. * correct / total)\n","\n","\n","def train_loop(\n","    model: nn.Module,\n","    criterion: nn.Module,\n","    checkpoint_name: str,\n","    model_name: str,\n","    experiment_name: str,\n","    train_data_loader: DataLoader,\n","    test_data_loader: DataLoader,\n","    teacher: Optional[nn.Module] = None,\n","    num_epochs: int = 5,\n","    learning_rate: float = 0.01,\n","    overwrite: bool = False,\n","    custom_train_epoch_fn = None,\n",") -> dict[str, list[float]]:\n","  \"\"\"Main training loop.\n","\n","  Args:\n","      model: Model to train.\n","      criterion: Loss function.\n","      checkpoint_name: Filename to save checkpoint.\n","      model_name: Display name for the model.\n","      experiment_name: Display name for the experiment.\n","      train_data_loader: Training loader.\n","      test_data_loader: Test loader.\n","      teacher: Optional teacher model.\n","      num_epochs: Number of epochs.\n","      learning_rate: Learning rate.\n","      overwrite: Whether to overwrite existing checkpoints.\n","\n","  Returns:\n","      dict[str, list[float]]: Training history.\n","  \"\"\"\n","  # Setup optimizer and scheduler\n","  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","  scheduler = optim.lr_scheduler.CosineAnnealingLR(\n","      optimizer, T_max=num_epochs, eta_min=0.001\n","  )\n","\n","  print(experiment_name)\n","  print(\"=\" * 60)\n","  history = collections.defaultdict(list)\n","  checkpoint_file_name = f'{checkpoint_name}.pth'\n","\n","  # Check if checkpoint exists safely\n","  if os.path.exists(checkpoint_file_name) and not overwrite:\n","    raise ValueError(f'The checkpoint {checkpoint_file_name} already exists!')\n","\n","  best_test_acc = 0.0\n","  for epoch in range(num_epochs):\n","      train_start = time.time()\n","\n","      # Run training epoch\n","      if custom_train_epoch_fn is None:\n","        metrics = train_epoch(\n","            model, teacher, train_data_loader, criterion, optimizer, device\n","        )\n","      else:\n","        metrics = custom_train_epoch_fn(\n","            model, teacher, train_data_loader, criterion, optimizer, device\n","        )\n","      train_time = time.time() - train_start\n","\n","      # Evaluation\n","      eval_start = time.time()\n","      metrics.update(evaluate(model, test_data_loader, device))\n","      eval_time = time.time() - eval_start\n","\n","      # Update learning rate\n","      scheduler.step()\n","\n","      # Store metrics\n","      for k, v in metrics.items():\n","          history[k].append(v)\n","\n","      train_acc = metrics.pop('train_acc')\n","      test_acc = metrics.pop('test_acc')\n","\n","      # Log progress\n","      print(\n","                f\"Epoch {epoch+1:2d}/{num_epochs}\",\n","                ' |'.join(\n","                    f\" {k}: {v:.4f}\"\n","                    for k, v in metrics.items()\n","                    if teacher is not None or k not in ['hard_loss', 'soft_loss']\n","                ),\n","                f\"| Train Acc {train_acc:.2f}% | Test Acc {test_acc:.2f}% \"\n","                f\"| Epoch time {train_time:.2f}s | Eval time {eval_time:.2f}s\"\n","            )\n","\n","      # Save best model\n","      if test_acc > best_test_acc:\n","          best_test_acc = test_acc\n","          torch.save(model.state_dict(), checkpoint_file_name)\n","\n","  print(f\"\\nâœ… {model_name} final test accuracy: {best_test_acc:.2f}%\")\n","  return history\n","\n","\n","def to_cpu(tensor_or_list):\n","  \"\"\"Converts GPU values to CPU ones. Useful for visualisation.\"\"\"\n","  return torch.tensor(tensor_or_list).cpu()"]},{"cell_type":"markdown","metadata":{"id":"_Fi3_7I1U2Kj"},"source":["## 6. Train the Teacher Model\n","\n","First, we need a well-trained teacher. In practice, you might use a pre-trained model, but we'll train one from scratch for educational purposes.\n","\n","**Note**: To save time, we'll provide the checkpoint, but you can check the training code and train your own teacher later."]},{"cell_type":"markdown","metadata":{"id":"b5ScyNaA0uWV"},"source":["### 6.1. Download the Teacher Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DzQd6zjx2QEk"},"outputs":[],"source":["!gdown --id 1Ko41G-TVerBr1tY0cSr4m1h1s9PvHRXw\n","\n","teacher.load_state_dict(torch.load('teacher_resnet18.pth'))\n","teacher.eval()"]},{"cell_type":"markdown","metadata":{"id":"xXEguB550pey"},"source":["### 6.2. Train the Teacher Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DBLrBq-qU2Kj"},"outputs":[],"source":["# # Training configuration\n","# TEACHER_EPOCHS = 15  # Increase for better teacher (50+ recommended)\n","# LEARNING_RATE = 0.01\n","\n","# teacher = TeacherCNN().to(device)\n","# teacher_history = train_loop(\n","#     model=teacher,\n","#     criterion=nn.CrossEntropyLoss(),\n","#     train_data_loader=trainloader,\n","#     test_data_loader=testloader,\n","#     num_epochs=TEACHER_EPOCHS,\n","#     learning_rate=LEARNING_RATE,\n","#     checkpoint_name=\"teacher_resnet18.pth\",\n","#     model_name=\"Teacher\",\n","#     experiment_name=\"Teacher Model (ResNet-18)\",\n","#     overwrite=False,\n","\n","# )"]},{"cell_type":"markdown","metadata":{"id":"DF7uzQ5WU2Kj"},"source":["## 7. Baseline. Student without distillation\n","\n","Let's start by training the student network with only hard labels. This will allow us to appreciate the value of distillation."]},{"cell_type":"markdown","source":["### Activity 2. Train Student WITHOUT Distillation"],"metadata":{"id":"j_JjX38qLsqM"}},{"cell_type":"code","source":["#@title Training Hypers\n","STUDENT_EPOCHS = 5\n","LR = 0.01"],"metadata":{"id":"7vu0o0ADNvWz"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ansQrnAHU2Kj"},"outputs":[],"source":["# Train student from scratch (no teacher)\n","teacher_dim = teacher.fc.in_features\n","student_baseline = StudentCNN(output_dim=teacher_dim).to(device)\n","\n","baseline_history = train_loop(\n","    model=student_baseline,\n","    criterion=nn.CrossEntropyLoss(),\n","    train_data_loader=trainloader,\n","    test_data_loader=testloader,\n","    num_epochs=STUDENT_EPOCHS,\n","    learning_rate=LR,\n","    checkpoint_name=\"baseline_student_v1\",\n","    model_name=\"Baseline Student\",\n","    experiment_name=\"Training Student Baseline (NO distillation)\",\n","    overwrite=False,\n","    # overwrite=True,\n",")"]},{"cell_type":"markdown","source":["### Solution (Download the student's checkpoint)"],"metadata":{"id":"CnCgIaFdL4_5"}},{"cell_type":"code","source":["# @title\n","!gdown --id 1xTYieLn5jpIb9SWplvMqrU3otCQC81Q-\n","\n","student_baseline.load_state_dict(torch.load('baseline_student_v1.pth'))"],"metadata":{"id":"O22A9t_RLi-u","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-smlZ6nEU2Kj"},"source":["## 8. The Core: Knowledge Distillation Loss\n","\n","### The Distillation Loss Function\n","The key innovation is combining two losses:\n","$$L_{total} = \\alpha \\cdot L_{hard} + (1 - \\alpha) \\cdot L_{soft}$$\n","Where:\n","- $L_{hard}$ = Cross-entropy with true labels (standard supervised loss)\n","- $L_{soft}$ = KL divergence between teacher and student soft predictions\n","- $\\alpha$ = Weight balancing the two losses (typically 0.1-0.5)"]},{"cell_type":"markdown","metadata":{"id":"7G2wB-ocL6uH"},"source":["### Activity 3. Implement the `DistillationLoss` class in accordance with the previously provided description.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WfucajXBMgCT"},"outputs":[],"source":["class DistillationLoss(nn.Module):\n","    \"\"\"Knowledge Distillation Loss.\n","\n","    Combines:\n","    1. Hard label loss: classification cross entropy.\n","    2. Soft label loss: KL divergence between the student and teacher logits.\n","\n","    The T^2 factor compensates for the gradient magnitude reduction when using\n","    temperature.\n","    \"\"\"\n","    def __init__(self, alpha: float):\n","        super().__init__()\n","        # TODO\n","\n","    def forward(\n","          self,\n","          student_logits: torch.Tensor,\n","          teacher_logits: torch.Tensor,\n","          labels: torch.Tensor\n","        ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","        # TODO\n","        metrics = dict(hard_loss=0., soft_loss=0.)\n","        return total_loss, metrics"]},{"cell_type":"markdown","metadata":{"id":"gUJR7uwBMa7n"},"source":["#### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yaju1hJFU2Kj","cellView":"form"},"outputs":[],"source":["# @title\n","class DistillationLoss(nn.Module):\n","    \"\"\"Knowledge Distillation Loss.\n","\n","    Combines:\n","    1. Hard label loss: classification cross entropy.\n","    2. Soft label loss: KL divergence between the student and teacher logits.\n","\n","    The T^2 factor compensates for the gradient magnitude reduction when using\n","    temperature.\n","    \"\"\"\n","    def __init__(self, alpha: float):\n","        super().__init__()\n","        self.alpha = alpha\n","        self.ce_loss = nn.CrossEntropyLoss()\n","        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n","\n","    def forward(\n","          self,\n","          student_logits: torch.Tensor,\n","          teacher_logits: torch.Tensor,\n","          labels: torch.Tensor\n","        ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","        # Hard label loss (standard cross-entropy)\n","        hard_loss = self.ce_loss(student_logits, labels)\n","\n","        # Soft label loss (KL divergence)\n","        # Student: log-softmax\n","        student_logits = F.log_softmax(student_logits, dim=1)\n","        # Teacher: softmax (target distribution)\n","        teacher_probs = F.softmax(teacher_logits, dim=1)\n","\n","        # KL divergence * T^2 (to match gradient magnitude)\n","        soft_loss = self.kl_loss(student_logits, teacher_probs)\n","\n","        # Combined loss\n","        total_loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss\n","\n","        metrics = dict(hard_loss=hard_loss, soft_loss=soft_loss)\n","        return total_loss, metrics"]},{"cell_type":"markdown","metadata":{"id":"IDlTPBQuU2Kj"},"source":["## 9. Train Student with Distillation"]},{"cell_type":"markdown","metadata":{"id":"JkpsqpVQOpJJ"},"source":["### Training with distillation (Using big network as teacher)"]},{"cell_type":"markdown","source":["#### Activity 4. Implement the training code that uses the distillation loss defined above to distill knowledge from a large pretrained teacher model to a smaller student model."],"metadata":{"id":"YlEB46UlXf-W"}},{"cell_type":"code","source":["# Distillation configuration\n","STUDENT_EPOCHS = 5\n","ALPHA = 0.1        # Weight for hard loss (try: 0.1, 0.3, 0.5)\n","LR = 0.01\n","\n","# Load trained teacher\n","# TODO\n","\n","# Load student\n","# TODO\n","\n","\n","# Define training loss\n","# TODO\n","\n","# Start the model training.\n","# TODO"],"metadata":{"id":"fyqHDc-wYb0-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Solution"],"metadata":{"id":"x5eXTxpfYXaf"}},{"cell_type":"code","source":["# @title\n","# Distillation configuration\n","STUDENT_EPOCHS = 5\n","ALPHA = 0.1        # Weight for hard loss (try: 0.1, 0.3, 0.5)\n","LR = 0.01\n","\n","# Load trained teacher\n","teacher.load_state_dict(torch.load('teacher_resnet18.pth'))\n","teacher.eval()\n","\n","# Load student\n","teacher_dim = teacher.fc.in_features\n","student_distilled = StudentCNN(output_dim=teacher_dim).to(device)\n","\n","# Define training loss\n","distill_criterion = DistillationLoss(alpha=ALPHA)\n","\n","# Start the model training.\n","distilled_history = train_loop(\n","    model=student_distilled,\n","    teacher=teacher,\n","    criterion=distill_criterion,\n","    train_data_loader=trainloader,\n","    test_data_loader=testloader,\n","    num_epochs=STUDENT_EPOCHS,\n","    learning_rate=LR,\n","    checkpoint_name=\"student_distilled_v1\",\n","    model_name=\"Distilled Student\",\n","    experiment_name=\"Training Student with Knowledge Distillation\",\n","    overwrite=False,\n","    # overwrite=True,\n",")"],"metadata":{"id":"QefQ8XKeEbyN","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZJ2EjM3zU2Kj"},"source":["## 10. Results Comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rnHzjg9PU2Kj"},"outputs":[],"source":["# Final comparison\n","teacher_acc = evaluate(teacher, testloader, device)['test_acc']\n","distilled_acc = evaluate(student_distilled, testloader, device)['test_acc']\n","baseline_acc = evaluate(student_baseline, testloader, device)['test_acc']\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"ðŸ“Š FINAL RESULTS\")\n","print(\"=\"*60)\n","print(f\"{'Model':<25} {'Parameters':<15} {'Test Accuracy':<15}\")\n","print(\"-\"*60)\n","print(f\"{'Teacher (ResNet-18)':<25} {teacher_params:>12,} {teacher_acc:>12.2f}%\")\n","print(f\"{'Student (Baseline)':<25} {student_params:>12,} {baseline_acc:>12.2f}%\")\n","print(f\"{'Student (Distilled)':<25} {student_params:>12,} {distilled_acc:>12.2f}%\")\n","print(\"-\"*60)\n","print(f\"\\nðŸ“ˆ Distillation improvement: +{distilled_acc - baseline_acc:.2f}%\")\n","print(f\"ðŸ—œï¸  Compression ratio: {teacher_params / student_params:.1f}x fewer parameters\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R2UF_dpoU2Kj","cellView":"form"},"outputs":[],"source":["#@title Plot training curves\n","fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n","\n","xs = np.arange(1, STUDENT_EPOCHS + 1)\n","# Test accuracy comparison\n","axes[0].plot(xs, baseline_history['test_acc'], label='Student (Baseline)', linestyle='--')\n","axes[0].plot(xs, distilled_history['test_acc'], label='Student (Distilled)', linewidth=2)\n","axes[0].axhline(y=teacher_acc, color='r', linestyle=':', label=f'Teacher ({teacher_acc:.1f}%)')\n","axes[0].set_xlabel('Epoch')\n","axes[0].set_ylabel('Test Accuracy (%)')\n","axes[0].set_title('Test Accuracy Comparison')\n","axes[0].legend()\n","axes[0].grid(True, alpha=0.3)\n","\n","# Loss breakdown for distilled student\n","axes[1].plot(xs, to_cpu(distilled_history['hard_loss']), label='Hard Loss (CE)', alpha=0.8)\n","axes[1].plot(xs, to_cpu(distilled_history['soft_loss']), label='Soft Loss (KL)', alpha=0.8)\n","axes[1].plot(xs, to_cpu(distilled_history['loss']), label='Total Loss', linewidth=2)\n","axes[1].set_xlabel('Epoch')\n","axes[1].set_ylabel('Loss')\n","axes[1].set_title('Distillation Loss Components')\n","axes[1].legend()\n","axes[1].grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"q69bvOz2U2Kj"},"source":["## 11. Inference Speed Comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YnZ4Ix0_nBrL"},"outputs":[],"source":["def benchmark_inference(\n","    model: nn.Module,\n","    input_size: tuple = (1, 3, 32, 32),\n","    num_iterations: int = 1000\n",") -> float:\n","    \"\"\"Benchmark inference speed.\n","\n","    Args:\n","        model (nn.Module): Model to benchmark.\n","        input_size (tuple): Input tensor shape.\n","        num_iterations (int): Number of iterations to average.\n","\n","    Returns:\n","        float: Average time per inference in milliseconds.\n","    \"\"\"\n","    model.eval()\n","    dummy_input = torch.randn(input_size).to(device)\n","\n","    # Warmup\n","    for _ in range(100):\n","        with torch.no_grad():\n","            _ = model(dummy_input)\n","\n","    # Benchmark\n","    if device.type == 'cuda':\n","        torch.cuda.synchronize()\n","\n","    start = time.time()\n","    for _ in range(num_iterations):\n","        with torch.no_grad():\n","            _ = model(dummy_input)\n","\n","    if device.type == 'cuda':\n","        torch.cuda.synchronize()\n","\n","    elapsed = time.time() - start\n","    return elapsed / num_iterations * 1000  # ms per inference\n","\n","teacher_time = benchmark_inference(teacher)\n","student_time = benchmark_inference(student_distilled)\n","\n","print(\"\\nâš¡ Inference Speed (single sample)\")\n","print(\"-\" * 40)\n","print(f\"Teacher (ResNet-18): {teacher_time:.3f} ms\")\n","print(f\"Student (Distilled): {student_time:.3f} ms\")\n","print(f\"Speedup: {teacher_time / student_time:.2f}x faster\")"]},{"cell_type":"markdown","metadata":{"id":"Ldi4RQTnnBrL"},"source":["## 12. Temperature Scaling\n","To transfer more of the dark knowledge from the teacher to the student, we \"soften\" the probability distributions using temperature $T$ applied to the logits $z$:\n","$$p_i = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}$$\n","- $T = 1$: Normal softmax\n","- $T > 1$: Softer distribution (reveals more information about class relationships)\n","- Typically $T \\in [3, 20]$\n","- The temperature is applied to both the student and the teacher logits."]},{"cell_type":"markdown","metadata":{"id":"QC2D5dwGnBrL"},"source":["### ðŸ” Let's Visualize Temperature Effects\n","\n","Understanding how temperature affects the probability distribution is crucial."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O_C9j-conBrL"},"outputs":[],"source":["# Demonstrate temperature effect\n","def visualize_temperature_effect(logits: torch.Tensor, temperatures: list = [1, 2, 4, 8, 16]) -> None:\n","    \"\"\"Visualize how temperature affects the softmax distribution.\n","\n","    Args:\n","        logits (torch.Tensor): Output logits from a model (1D tensor).\n","        temperatures (list): List of temperatures to visualize.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    fig, axes = plt.subplots(1, len(temperatures), figsize=(15, 3))\n","\n","    for ax, T in zip(axes, temperatures):\n","        probs = F.softmax(logits / T, dim=0).numpy()\n","        ax.bar(range(len(probs)), probs, color='steelblue')\n","        ax.set_title(f'T = {T}')\n","        ax.set_xlabel('Class')\n","        ax.set_ylabel('Probability')\n","        ax.set_ylim(0, 1)\n","        ax.set_xticks(range(10))\n","\n","    plt.suptitle('Effect of Temperature on Softmax Distribution', fontsize=14)\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Example logits (model is quite confident about class 3)\n","example_logits = torch.tensor([1.0, 2.0, 1.5, 8.0, 0.5, 2.5, 1.0, 0.8, 1.2, 1.8])\n","visualize_temperature_effect(example_logits)\n","\n","print(\"\\nðŸ“Š Observation:\")\n","print(\"- T=1: Sharp distribution (almost one-hot)\")\n","print(\"- T>1: Softer distribution revealing class relationships\")\n","print(\"- Higher T = more 'dark knowledge' transferred\")"]},{"cell_type":"markdown","source":["### Activity 5: How do loss gradients change with the temperature?\n","Remember that the temperature is applied to both the student and the teacher predictions.\n","Your task:\n","1. Take the example teacher and student logits below.\n","2. Write a function to compute the loss and the gradient with respect to the student logits. Hint: the loss is computed in the DistillationLoss class; for gradients you need to do `tensor.backward()` and then you can access the gradient by `tensor.grad`\n","3. Apply temperature to both the student and the teacher logits.\n","4. Comptute loss and gradient values for different levels of temperature. How does the gradient change?\n","\n","Is the changing gradient a problem? What can you do about it?"],"metadata":{"id":"4fsoBWdn7EKp"}},{"cell_type":"code","source":["# Example student and teacher logits\n","student_logits = torch.from_numpy(np.asarray([\n","    -0.21406781, -1.47030687, -0.52430741,  0.88407076,  3.20583727,\n","    0.27593067, -0.54877487,  0.9942204 ,  0.11794449,  0.63376627\n","]))\n","\n","teacher_logits = torch.from_numpy(np.asarray([\n","    0.64185687,  1.52115532, -0.7592614 , -2.06392082,  0.49512868,\n","    0.74697671, -0.19236562,  1.91342173,  0.78049914,  0.00639535\n","]))"],"metadata":{"id":"g9WWdbA07gsX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_kl_gradient(student_logits: torch.Tensor, teacher_logits: torch.Tensor, temperature: float = 1.0):\n","    # Ensure student_logits requires gradients\n","    student_logits.requires_grad_(True)\n","\n","    # TODO\n","\n","\n","    # Return the gradients of student_logits\n","    return grad"],"metadata":{"id":"nl_uDtNy7lyF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Solution"],"metadata":{"id":"Ovob2UjH7q5x"}},{"cell_type":"code","source":["# @title\n","def compute_kl_gradient(student_logits: torch.Tensor, teacher_logits: torch.Tensor, temperature: float = 1.0):\n","    # Ensure student_logits requires gradients\n","    student_logits.requires_grad_(True)\n","\n","    # Apply temperature to logits\n","    student_tempered_logits = student_logits / temperature\n","    teacher_tempered_logits = teacher_logits / temperature\n","\n","    # Compute log-softmax for student and softmax for teacher\n","    student_log_probs = F.log_softmax(student_tempered_logits, dim=0)\n","    teacher_probs = F.softmax(teacher_tempered_logits, dim=0)\n","\n","    # Compute KL divergence loss\n","    loss = nn.KLDivLoss(reduce='mean')(student_log_probs, teacher_probs)\n","\n","    # Compute gradients\n","    loss.backward()\n","\n","    # Return the gradients of student_logits\n","    return student_logits.grad"],"metadata":{"cellView":"form","id":"FKdR1wRU8W_C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Let's compute some gradients and visualize"],"metadata":{"id":"BRXPdDuiNMNB"}},{"cell_type":"code","source":["temperatures = [1.0, 2.0, 4.0, 8.0]\n","gradients = []\n","for T in temperatures:\n","    # Clone logits to ensure fresh gradient computation for each iteration\n","    student_logits_clone = student_logits.clone().detach()\n","    teacher_logits_clone = teacher_logits.clone().detach()\n","\n","    grad = compute_kl_gradient(student_logits_clone, teacher_logits_clone, T)\n","    gradients.append(torch.norm(grad))"],"metadata":{"id":"dvDT-eTsPKo9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Visualize\n","# Convert gradient norms to numpy for plotting if they are tensors\n","plot_gradient_norms = [gn.item() for gn in gradients]\n","\n","fig, ax = plt.subplots(figsize=(8, 5))\n","\n","# Plot gradient norm vs temperature\n","ax.plot(temperatures, plot_gradient_norms, marker='o', linestyle='-', color='b', label='Observed Gradient Norm')\n","\n","ax.set_xlabel('Temperature (T)')\n","ax.set_ylabel('Value')\n","ax.set_title('Gradient Norm vs. Temperature')\n","ax.legend()\n","ax.grid(True, linestyle=':', alpha=0.7)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"cellView":"form","id":"4C2MYa82NR9A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Scales for gradients"],"metadata":{"id":"R5I2ph6dNfCu"}},{"cell_type":"code","source":["# TODO: Change these four numbers to make sure the gradients don't\n","# change too much with temperatures\n","scales_for_gradients = [1, 1, 1, 1]"],"metadata":{"id":"i22JrMG6NkYZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Solution"],"metadata":{"id":"88sIyNqxOFlw"}},{"cell_type":"code","source":["# @title\n","scales_for_gradients = [t**2 for t in temperatures]"],"metadata":{"cellView":"form","id":"JuXXKZd_OHI3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Visualise Scaled Gradients\n","# Convert gradient norms to numpy for plotting if they are tensors\n","plot_gradient_norms = [gn.item() for gn in gradients]\n","\n","fig, ax = plt.subplots(figsize=(8, 5))\n","\n","# Plot gradient norm vs temperature\n","ax.plot(temperatures, plot_gradient_norms, marker='o', linestyle='-', color='b', label='Observed Gradient Norm')\n","\n","# Plot scaled_gradients\n","t_values_for_plot = np.array(temperatures)\n","inv_t_squared = np.asarray(plot_gradient_norms) * np.asarray(scales_for_gradients)\n","ax.plot(temperatures, inv_t_squared, linestyle='--', color='r', label='Scaled Gradient Norm')\n","\n","ax.set_xlabel('Temperature (T)')\n","ax.set_ylabel('Value')\n","ax.set_title('Gradient Norm vs. Temperature with scaling')\n","ax.legend()\n","ax.grid(True, linestyle=':', alpha=0.7)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"cellView":"form","id":"EBcxOZo_857f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As the temperature T increases, the gradient of the loss gets rescaled by a factor of (1/T)^2. This has been derived in the original Hinton paper. As decreasing gradient value can affect optimization dynamics, we need to upscale the gradients by a factor of T^2. The easiest way to do it is just to multiply the loss by T^2.\n","\n","Note: we are talking about distillation for classification, but it can be applied to other tasks. This temperature-scaling result holds only for classification."],"metadata":{"id":"LZSkIdTZ7eWD"}},{"cell_type":"markdown","metadata":{"id":"6U1-buA-nBrL"},"source":["### Activity 6. Implement the `TemperedDistillationLoss` class.\n","It should be similar to `DistillationLoss` but with the temperature scaling applied to the teacher and student probabilities. The loss should be rescaled by temperature.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTeZL7pwnBrL"},"outputs":[],"source":["class TemperedDistillationLoss(nn.Module):\n","    \"\"\"Knowledge Distillation Loss with Temperature Scaling.\n","\n","    Combine the hard loss computed on labels with the soft teacher loss. Use\n","    Alpha weight on the hard loss and (1 - alpha) on the soft loss.\n","    \"\"\"\n","    def __init__(self, temperature: float, alpha: float) -> None:\n","        \"\"\"Initialize TemperedDistillationLoss.\n","\n","        Args:\n","            temperature (float): Temperature for softening the distribution.\n","            alpha (float): Weight for hard loss (1-alpha for soft loss).\n","        \"\"\"\n","        super().__init__()\n","        # TODO\n","\n","    def forward(\n","        self,\n","        student_logits: torch.Tensor,\n","        teacher_logits: torch.Tensor,\n","        labels: torch.Tensor\n","    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","        \"\"\"Compute the tempered distillation loss.\n","\n","        Args:\n","            student_logits: Logits from student model.\n","            teacher_logits: Logits from teacher model.\n","            labels: True labels.\n","\n","        Returns:\n","            tuple: (total_loss, hard_loss, soft_loss)\n","        \"\"\"\n","        # TODO\n","        metrics = dict(hard_loss=0., soft_loss=0)\n","        return total_loss, metrics"]},{"cell_type":"markdown","metadata":{"id":"_2xr9dKsnBrL"},"source":["#### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BZ0j8wK7nBrL","cellView":"form"},"outputs":[],"source":["# @title\n","class TemperedDistillationLoss(nn.Module):\n","    \"\"\"Knowledge Distillation Loss with Temperature Scaling.\n","\n","    Combines:\n","    1. Hard label loss: CrossEntropy(student_output, true_labels)\n","    2. Soft label loss: KLDiv(student_soft, teacher_soft) * T^2\n","\n","    The T^2 factor compensates for the gradient magnitude reduction when using\n","    temperature.\n","    \"\"\"\n","    def __init__(self, temperature: float = 4.0, alpha: float = 0.3) -> None:\n","        \"\"\"Initialize TemperedDistillationLoss.\n","\n","        Args:\n","            temperature (float): Temperature for softening the distribution.\n","            alpha (float): Weight for hard loss (1-alpha for soft loss).\n","        \"\"\"\n","        super().__init__()\n","        self.temperature = temperature\n","        self.alpha = alpha\n","        self.ce_loss = nn.CrossEntropyLoss()\n","        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n","\n","    def forward(\n","        self,\n","        student_logits: torch.Tensor,\n","        teacher_logits: torch.Tensor,\n","        labels: torch.Tensor\n","    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","        \"\"\"Compute the tempered distillation loss.\n","\n","        Args:\n","            student_logits: Logits from student model.\n","            teacher_logits: Logits from teacher model.\n","            labels: True labels.\n","\n","        Returns:\n","            tuple: (total_loss, hard_loss, soft_loss)\n","        \"\"\"\n","        # Hard label loss (standard cross-entropy)\n","        hard_loss = self.ce_loss(student_logits, labels)\n","\n","        # Soft label loss (KL divergence with temperature)\n","        # Student: log-softmax with temperature\n","        student_soft = F.log_softmax(student_logits / self.temperature, dim=1)\n","        # Teacher: softmax with temperature (target distribution)\n","        teacher_soft = F.softmax(teacher_logits / self.temperature, dim=1)\n","\n","        # KL divergence * T^2 (to match gradient magnitude)\n","        soft_loss = self.kl_loss(student_soft, teacher_soft) * (self.temperature ** 2)\n","\n","        # Combined loss\n","        total_loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss\n","\n","        metrics = dict(hard_loss=hard_loss, soft_loss=soft_loss)\n","        return total_loss, metrics"]},{"cell_type":"markdown","source":["### Training with temperature-scaled distillation (Using big network as teacher)"],"metadata":{"id":"W2ee5vCuqHK1"}},{"cell_type":"code","source":["# Distillation configuration\n","ALPHA = 0.1        # Weight for hard loss (try: 0.1, 0.3, 0.5)\n","TEMPERATURE = 4.0  # Try values in [1, 20]\n","LR = 0.01\n","\n","\n","# Load trained teacher\n","teacher.load_state_dict(torch.load('teacher_resnet18.pth'))\n","teacher.eval()\n","\n","teacher_dim = teacher.fc.in_features\n","student_tempered = StudentCNN(output_dim=teacher_dim).to(device)\n","\n","# Setup training\n","tempered_distill_criterion = TemperedDistillationLoss(\n","    temperature=TEMPERATURE,\n","    alpha=ALPHA\n",")\n","\n","tempered_distillation_history = train_loop(\n","    model=student_tempered,\n","    teacher=teacher,\n","    criterion=tempered_distill_criterion,\n","    train_data_loader=trainloader,\n","    test_data_loader=testloader,\n","    num_epochs=STUDENT_EPOCHS,\n","    learning_rate=LR,\n","    checkpoint_name=\"student_distilled_tempered_v1\",\n","    model_name=\"Tempered-distillation Student\",\n","    experiment_name=\"Training Student with Tempered Knowledge Distillation\",\n","    overwrite=True,\n",")"],"metadata":{"id":"A0JJ57drqHsQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Plot training curves\n","fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n","\n","# Test accuracy comparison (axes[0] remains as is, it already has distinct labels and lines)\n","xs = np.arange(1, len(baseline_history['test_acc']) + 1)\n","axes[0].plot(xs, baseline_history['test_acc'], label='Student (Baseline)', linestyle='--')\n","axes[0].plot(xs, distilled_history['test_acc'], label='Student (Distilled)', linewidth=2)\n","axes[0].plot(xs, tempered_distillation_history['test_acc'], label='Student (Tempered Distillation)', linewidth=2)\n","axes[0].axhline(y=teacher_acc, color='r', linestyle=':', label=f'Teacher ({teacher_acc:.1f}%)')\n","axes[0].set_xlabel('Epoch')\n","axes[0].set_ylabel('Test Accuracy (%)')\n","axes[0].set_title('Test Accuracy Comparison')\n","axes[0].legend()\n","axes[0].grid(True, alpha=0.3)\n","\n","# Loss breakdown for distilled student (axes[1])\n","# Define consistent colors for the loss components\n","hard_loss_color = 'tab:blue'\n","soft_loss_color = 'tab:orange'\n","total_loss_color = 'tab:green'\n","\n","# Plot original distillation losses with solid lines\n","axes[1].plot(xs, to_cpu(distilled_history['hard_loss']), label='Hard Loss (Distilled)', alpha=0.8, color=hard_loss_color)\n","axes[1].plot(xs, to_cpu(distilled_history['soft_loss']), label='Soft Loss (Distilled)', alpha=0.8, color=soft_loss_color)\n","axes[1].plot(xs, to_cpu(distilled_history['loss']), label='Total Loss (Distilled)', linewidth=2, color=total_loss_color)\n","\n","# Plot tempered distillation losses with dashed lines and the same colors\n","axes[1].plot(xs, to_cpu(tempered_distillation_history['hard_loss']), alpha=0.8, color=hard_loss_color, linestyle='--')\n","axes[1].plot(xs, to_cpu(tempered_distillation_history['soft_loss']), alpha=0.8, color=soft_loss_color, linestyle='--')\n","axes[1].plot(xs, to_cpu(tempered_distillation_history['loss']), linewidth=2, color=total_loss_color, linestyle='--')\n","axes[1].plot([], [], linewidth=2, linestyle='--', c='k', label='Tempered')\n","\n","axes[1].set_xlabel('Epoch')\n","axes[1].set_ylabel('Loss')\n","axes[1].set_title('Distillation Loss Components')\n","axes[1].legend()\n","axes[1].grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"rjsJgnAyqRdT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 13. Distillation Aids Generalization\n","Sometimes you may have a large model pre-trained on a large dataset A, but you may want to train a different model on a different, often smaller, dataset B. If the two datasets are related, it may be possible to transfer knowledge from one to the other.\n","\n","To do that, you take the teacher model trained on the dataset A and distill it into the student trained on the dataset B. In this case, the teacher is evaluated on the B dataset during distillation.\n","\n","To simulate this scenario, we will take a teacher trained on Cifar-10 (our dataset A) and distill it into a student trained on a modified version of Cifar-19 (dataset B).\n","\n","Let's start with modifying the Cifar-10 dataset."],"metadata":{"id":"vo30wHxYvdru"}},{"cell_type":"code","source":["# Conver Cifar-10 to grayscale and add noise (old camera simulation)\n","class AddGaussianNoise:\n","    \"\"\"Add Gaussian noise to a tensor.\"\"\"\n","    def __init__(self, mean: float = 0., std: float = 0.1) -> None:\n","        \"\"\"Initialize the transform.\n","\n","        Args:\n","            mean (float): Mean of the noise.\n","            std (float): Standard deviation of the noise.\n","        \"\"\"\n","        self.mean = mean\n","        self.std = std\n","\n","    def __call__(self, tensor: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Apply noise to the tensor.\n","\n","        Args:\n","            tensor (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Noisy tensor.\n","        \"\"\"\n","        noise = torch.randn_like(tensor) * self.std + self.mean\n","        return torch.clamp(tensor + noise, 0., 1.)\n","\n","transform_train_grayscale_noisy = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.Grayscale(num_output_channels=3),\n","    transforms.ToTensor(),\n","    AddGaussianNoise(mean=0., std=0.02),\n","    transforms.Normalize(cifar_mean, cifar_std),\n","])\n","\n","transform_test_grayscale_noisy = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=3),\n","    transforms.ToTensor(),\n","    AddGaussianNoise(mean=0., std=0.02),\n","    transforms.Normalize(cifar_mean, cifar_std),\n","])\n","\n","# Download and load datasets\n","augmented_cifar10_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train_grayscale_noisy)\n","augmented_cifar10_test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test_grayscale_noisy)\n","\n","augmented_cifar10_trainloader = DataLoader(augmented_cifar10_train, batch_size=128, shuffle=True, num_workers=0)\n","augmented_cifar10_testloader = DataLoader(augmented_cifar10_test, batch_size=128, shuffle=False, num_workers=0)"],"metadata":{"id":"FWzuSnVRvqIj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Activity 7. Plot some instances\n","Similar to the original CIFAR-10 dataset, plot a few sample instances to assess the impact of the applied transformation."],"metadata":{"id":"5N4kYrCiwDv8"}},{"cell_type":"code","source":["# Get some random training images\n","dataiter_v2 = iter(augmented_cifar10_trainloader)\n","images, labels = next(dataiter_v2)\n","\n","# Show images\n","# TODO"],"metadata":{"id":"CQGqceqRwAbm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Solution"],"metadata":{"id":"teUFxYwHwLcI"}},{"cell_type":"code","source":["# @title\n","# Get some random training images\n","dataiter_v2 = iter(augmented_cifar10_trainloader)\n","images, labels = next(dataiter_v2)\n","\n","# Show images\n","fig, axes = plt.subplots(1, 8, figsize=(6, 1))\n","for i in range(8):\n","    imshow(axes[i], images[i])\n","    axes[i].set_title(classes[labels[i]])\n","    axes[i].axis('off')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"Oh1TyYjRwAeP","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Activity 8. Train the student without distillation on the new dataset."],"metadata":{"id":"LGBnuM8Pw8j6"}},{"cell_type":"code","source":["# TODO"],"metadata":{"id":"O1BT6BgzwAgw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Solution"],"metadata":{"id":"oxrjLMAfw-3t"}},{"cell_type":"code","source":["# @title\n","# Train student from scratch (no teacher)\n","STUDENT_EPOCHS = 5\n","LR = 0.01\n","\n","teacher_dim = teacher.fc.in_features\n","aug_cifar10_student_baseline = StudentCNN(output_dim=teacher_dim).to(device)\n","\n","aug_cifar10_student_baseline_history = train_loop(\n","    model=aug_cifar10_student_baseline,\n","    teacher=None,\n","    criterion=nn.CrossEntropyLoss(),\n","    train_data_loader=augmented_cifar10_trainloader,\n","    test_data_loader=augmented_cifar10_testloader,\n","    num_epochs=STUDENT_EPOCHS,\n","    learning_rate=LR,\n","    checkpoint_name=\"augmented_cifar_student_no_distillation\",\n","    model_name=\"Augmented Cifar-10 Baseline\",\n","    experiment_name=\"Training Student Baseline (NO distillation)\",\n","    overwrite=True,\n",")"],"metadata":{"id":"_gNImKg3wAjj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Download the checkpoint"],"metadata":{"id":"Gt-IuNnE7m_-"}},{"cell_type":"code","source":["!gdown --id 19vQMX9RdA7LRq6gssRL8FVlapcv0KNRx\n","\n","aug_cifar10_student_baseline = StudentCNN(output_dim=teacher_dim).to(device)\n","aug_cifar10_student_baseline.load_state_dict(torch.load('augmented_cifar_student_no_distillation.pth'))"],"metadata":{"id":"H0GvwJzb8Dnu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Activity 9. Train the student network on the new dataset with distillation\n","We will use the TemperedDistillationLoss class, and compare the results against those obtained from training the student model independently."],"metadata":{"id":"-WsPJLzMxGVr"}},{"cell_type":"code","source":["# TODO"],"metadata":{"id":"fTwpjrgRxKrf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Solution"],"metadata":{"id":"kwGAzz5BxInx"}},{"cell_type":"code","source":["# @title\n","# Distillation configuration\n","STUDENT_EPOCHS = 5\n","TEMPERATURE = 4.0  # Try: 2, 4, 8, 20\n","ALPHA = 0.1        # Weight for hard loss (try: 0.1, 0.3, 0.5)\n","LR = 0.01\n","\n","# Load trained teacher\n","teacher.load_state_dict(torch.load('teacher_resnet18.pth'))\n","teacher.eval()\n","\n","teacher_dim = teacher.fc.in_features\n","aug_cifar10_student_distilled = StudentCNN(output_dim=teacher_dim).to(device)\n","distill_criterion = TemperedDistillationLoss(temperature=TEMPERATURE, alpha=ALPHA)\n","\n","aug_cifar10_student_distilled_history = train_loop(\n","    model=aug_cifar10_student_distilled,\n","    teacher=teacher,\n","    criterion=distill_criterion,\n","    train_data_loader=augmented_cifar10_trainloader,\n","    test_data_loader=augmented_cifar10_testloader,\n","    num_epochs=STUDENT_EPOCHS,\n","    learning_rate=LR,\n","    checkpoint_name=\"augmented_cifar_distilled_student\",\n","    model_name=\"Augmented Cifar-10 Distilled Student\",\n","    experiment_name=\"Training Distilled student on augmented Cifar-10\",\n","    overwrite=True,\n",")"],"metadata":{"id":"JYtYKifWxM0X","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Plot training curves\n","fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n","\n","xs = np.arange(1, STUDENT_EPOCHS + 1)\n","# Test accuracy comparison\n","axes[0].plot(xs, aug_cifar10_student_baseline_history['test_acc'], label='Student (Baseline)', linestyle='--')\n","axes[0].plot(xs, aug_cifar10_student_distilled_history['test_acc'], label='Student (Distilled)', linewidth=2)\n","axes[0].set_xlabel('Epoch')\n","axes[0].set_ylabel('Test Accuracy (%)')\n","axes[0].set_title('Test Accuracy Comparison')\n","axes[0].legend()\n","axes[0].grid(True, alpha=0.3)\n","\n","hard_loss_col = \"tab:blue\"\n","# Loss breakdown for distilled student\n","axes[1].plot(xs, to_cpu(aug_cifar10_student_distilled_history['hard_loss']), label='Hard Loss (CE)', alpha=0.8)\n","axes[1].plot(xs, to_cpu(aug_cifar10_student_distilled_history['soft_loss']), label='Soft Loss (KL)', alpha=0.8)\n","axes[1].plot(xs, to_cpu(aug_cifar10_student_distilled_history['loss']), label='Total Loss', linewidth=2)\n","axes[1].plot(\n","    xs, aug_cifar10_student_baseline_history['loss'],\n","    label='Hard Loss (Baseline)', alpha=0.8, c=hard_loss_col, linestyle='--'\n",")\n","axes[1].set_xlabel('Epoch')\n","axes[1].set_ylabel('Loss')\n","axes[1].set_title('Distillation Loss Components')\n","axes[1].legend()\n","axes[1].grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"u0d0G37451SC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 14. Multi-Expert Distillation\n","\n","To improve model performance, you can train several neural networks and use them together as an ensemble. A prediction from an ensemble $$x_\\text{ensemble} = \\frac{1}{n}\\sum_{i=1}^{n}x_i$$ is an average of the individual model predictions $x_i$.\n","\n","An ensemble is usually stronger than each of the individual models. It turns out that we can distill not just one model but the whole ensemble into the student network. That's what we will do now."],"metadata":{"id":"hV1AHACCBNqj"}},{"cell_type":"code","source":["teacher2 = TeacherCNN().to(device)"],"metadata":{"id":"tbXA4SLKMHqM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download the second teacher ckpt\n","!gdown --id 1P7wadGFV7ZPdJsXd2B91nKEEnQOst00G\n","\n","teacher2.load_state_dict(torch.load('teacher_resnet18_v2.pth'))\n","teacher2.eval()"],"metadata":{"id":"Jd-c0bJlKeBu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Alternative: Train a new teacher"],"metadata":{"id":"Wzq241dpKhN6"}},{"cell_type":"code","source":["# @title\n","# Set a different seed and train a new teacher\n","set_seed(20)\n","\n","# Training configuration\n","TEACHER_EPOCHS = 15  # Increase for better teacher (50+ recommended)\n","LEARNING_RATE = 0.01\n","\n","teacher2_history = train_loop(\n","    model=teacher2,\n","    criterion=nn.CrossEntropyLoss(),\n","    train_data_loader=trainloader,\n","    test_data_loader=testloader,\n","    num_epochs=TEACHER_EPOCHS,\n","    learning_rate=LEARNING_RATE,\n","    checkpoint_name=\"teacher_resnet18_v2.pth\",\n","    model_name=\"Teacher\",\n","    experiment_name=\"Teacher Model (ResNet-18)\",\n","    overwrite=False,\n","\n",")\n","\n","set_seed(10)"],"metadata":{"id":"jIkGbMrZB1JF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Activity 10. Modify `train_epoch` to accept a list of teachers.\n","For each teacher, compute the soft loss, then average them to get the final distillation loss."],"metadata":{"id":"Cso9vP7GCsJ7"}},{"cell_type":"code","source":["def train_epoch_multi_teacher(model, teacher, trainloader, criterion, optimizer, device):\n","    # TODO\n","    pass"],"metadata":{"id":"b7qhHkStHBfT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Solution"],"metadata":{"id":"bgmv7hkdG8nU"}},{"cell_type":"code","source":["# @title\n","def train_epoch_multi_teacher(model, teacher, trainloader, criterion, optimizer, device):\n","    \"\"\"Trains the model using supervised training or distillation.\n","\n","    Distillation is used only when the teacher is present. In that case, the\n","    teacher is frozen (no gradients).\n","    \"\"\"\n","    model.train()\n","\n","\n","    running_loss = 0.0\n","    running_hard_loss = 0.0\n","    running_soft_loss = 0.0\n","    correct = 0\n","    total = 0\n","    running_metrics = collections.defaultdict(int)\n","\n","    for inputs, labels in tqdm.tqdm(trainloader, desc=\"Training\", leave=False):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # Forward pass for the trained model\n","        optimizer.zero_grad()\n","        logits = model(inputs)\n","\n","        # Compute loss\n","        if teacher is not None:\n","            # Get teacher predictions (no gradient needed)\n","            if isinstance(teacher, list):\n","                avg_soft_loss = 0.0\n","                for t_model in teacher:\n","                    t_model.eval()\n","                    with torch.no_grad():\n","                        teacher_logits = t_model(inputs)\n","                    _, metrics = criterion(\n","                        logits, teacher_logits, labels\n","                    )\n","                    avg_soft_loss += metrics.pop('soft_loss')\n","                avg_soft_loss /= len(teacher)\n","                metrics['soft_loss'] = avg_soft_loss\n","                # hard loss is the same for every teacher\n","                loss = criterion.alpha * metrics['hard_loss'] + (1 - criterion.alpha) * avg_soft_loss\n","            else:\n","                teacher.eval()  # Teacher is always in eval mode\n","                with torch.no_grad():\n","                    teacher_logits = teacher(inputs)\n","                loss, metrics = criterion(\n","                    logits, teacher_logits, labels\n","                )\n","        else:\n","          loss = criterion(logits, labels)\n","          metrics = {}\n","\n","        # Backward and optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Metrics\n","        running_metrics['loss'] += loss.item()\n","        for k, v in metrics.items():\n","            running_metrics[k] += v.item()\n","        _, predicted = logits.max(1)\n","        total += labels.size(0)\n","        correct += predicted.eq(labels).sum().item()\n","\n","    n = len(trainloader)\n","    results = {k: v/n for k, v in running_metrics.items()}\n","    results['train_acc'] = 100. * correct / total\n","    return results"],"metadata":{"id":"hWnmEbzAEXze"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Activity 11. Train the student using the 2 experts"],"metadata":{"id":"1K2azWEFHL-q"}},{"cell_type":"code","source":["# TODO\n","multi_teacher_history = None"],"metadata":{"id":"UOjSMTEGKDrV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Solution"],"metadata":{"id":"z39l9bUFKACf"}},{"cell_type":"code","source":["# @title\n","# Distillation configuration\n","STUDENT_EPOCHS = 5\n","ALPHA = 0.1        # Weight for hard loss (try: 0.1, 0.3, 0.5)\n","TEMPERATURE = 4.0  # Try values in [1, 20]\n","LR = 0.01\n","\n","\n","# Load trained teacher\n","teacher1 = TeacherCNN().to(device)\n","teacher1.load_state_dict(torch.load('teacher_resnet18.pth'))\n","teacher1.eval()\n","teacher2 = TeacherCNN().to(device)\n","teacher2.load_state_dict(torch.load('teacher_resnet18_v2.pth'))\n","teacher2.eval()\n","\n","teacher_dim = teacher.fc.in_features\n","student_multi_distilled = StudentCNN(output_dim=teacher_dim).to(device)\n","\n","# Setup training\n","tempered_distill_criterion = TemperedDistillationLoss(\n","    temperature=TEMPERATURE,\n","    alpha=ALPHA\n",")\n","\n","multi_teacher_history = train_loop(\n","    model=student_multi_distilled,\n","    teacher=[teacher1, teacher2],\n","    criterion=tempered_distill_criterion,\n","    train_data_loader=trainloader,\n","    test_data_loader=testloader,\n","    num_epochs=STUDENT_EPOCHS,\n","    learning_rate=LR,\n","    checkpoint_name=\"student_multi_distilled\",\n","    model_name=\"Multi-Teacher Student\",\n","    experiment_name=\"Training Student with Tempered Multi-Expert Distillation\",\n","    overwrite=True,\n","    # Use a different train_epoch_function\n","    custom_train_epoch_fn=train_epoch_multi_teacher,\n",")"],"metadata":{"id":"fVJ5YPMyG7c2","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Plot training curves\n","fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n","\n","xs = np.arange(1, STUDENT_EPOCHS + 1)\n","# Test accuracy comparison\n","axes[0].plot(xs, tempered_distillation_history['test_acc'], label='Single-Teacher Student', linestyle='--')\n","axes[0].plot(xs, multi_teacher_history['test_acc'], label='Multi-Teacher Student', linewidth=2)\n","axes[0].set_xlabel('Epoch')\n","axes[0].set_ylabel('Test Accuracy (%)')\n","axes[0].set_title('Test Accuracy Comparison')\n","axes[0].legend()\n","axes[0].grid(True, alpha=0.3)\n","\n","hard_loss_color = 'tab:blue'\n","soft_loss_color = 'tab:orange'\n","total_loss_color = 'tab:green'\n","# Loss breakdown for distilled student\n","axes[1].plot(xs, to_cpu(multi_teacher_history['hard_loss']), label='Hard Loss (CE)', alpha=0.8, c=hard_loss_color)\n","axes[1].plot(xs, to_cpu(multi_teacher_history['soft_loss']), label='Soft Loss (KL)', alpha=0.8, c=soft_loss_color)\n","axes[1].plot(xs, to_cpu(multi_teacher_history['loss']), label='Total Loss', linewidth=2, c=total_loss_color)\n","axes[1].plot(xs, to_cpu(tempered_distillation_history['hard_loss']), linestyle='--', alpha=0.8, c=hard_loss_color)\n","axes[1].plot(xs, to_cpu(tempered_distillation_history['soft_loss']), linestyle='--', alpha=0.8, c=soft_loss_color)\n","axes[1].plot(xs, to_cpu(tempered_distillation_history['loss']), linestyle='--', linewidth=2, c=total_loss_color)\n","axes[1].plot([], [], linestyle='--', linewidth=2, c='k', label='Single Teacher')\n","\n","\n","axes[1].plot(\n","    xs, aug_cifar10_student_baseline_history['loss'],\n","    label='Hard Loss (Baseline)', alpha=0.8, c=hard_loss_col, linestyle='--'\n",")\n","\n","\n","axes[1].set_xlabel('Epoch')\n","axes[1].set_ylabel('Loss')\n","axes[1].set_title('Distillation Loss Components')\n","axes[1].legend()\n","axes[1].grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"cellView":"form","id":"Oyhjjq73luxz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 15. Feature Distillation\n","In addition to distilling features, we'll be looking at distilling intermediate features produced by the neural network.\n","\n","This technique allows more efficient transfer of the \"hidden\" knowledge, but it is more complicated. As the teacher may have a different architecture than the student, we need to match a teacher layer that we want to distill from with a student layer that we want to distill to. These layers may have different dimensionalities, so we might need to adapter the student features. We can use a linear layer to do so.\n","\n","The loss used for feature distilation is usually based on cosine similarity. Since we want something that gets better as it gets smaller, we take 1 - cosine similarity."],"metadata":{"id":"cO1Pn9Q-1sNn"}},{"cell_type":"markdown","source":["### Activity 12: Implement feature distillation loss"],"metadata":{"id":"aTUGq9Ga2RQa"}},{"cell_type":"code","source":["class FeatureDistillationLoss(nn.Module):\n","    \"\"\"\n","    Feature-based distillation: match intermediate representations.\n","    Requires a projection layer if dimensions don't match.\n","    \"\"\"\n","    def __init__(self, device, student_dim, teacher_dim, temperature=4.0, alpha=0.3, beta=0.5):\n","        super().__init__()\n","        self.temperature = temperature\n","        self.alpha = alpha\n","        self.beta = beta  # Weight for feature loss\n","\n","        # Projection layer if dimensions don't match\n","        self.projector = nn.Linear(student_dim, teacher_dim) if student_dim != teacher_dim else nn.Identity()\n","        self.projector = self.projector.to(device)\n","\n","        self.ce_loss = nn.CrossEntropyLoss()\n","        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n","\n","    def forward(self, student_logits, teacher_logits, student_features, teacher_features, labels):\n","        # TODO implement tempered-distillation loss we used before and combine\n","        # it with a new feature distillation loss. Read the above section for\n","        # how to implement it.\n","\n","\n","        metrics = dict(\n","            hard_loss=0.,\n","            soft_loss=0.,\n","            feature_loss=0.\n","        )\n","        return loss, metrics"],"metadata":{"id":"J2mk7buc2XbA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Solution"],"metadata":{"id":"xf3Hu31J2Ulg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hMPhV21tU2Kk","cellView":"form"},"outputs":[],"source":["# @title\n","class FeatureDistillationLoss(nn.Module):\n","    \"\"\"\n","    Feature-based distillation: match intermediate representations.\n","    Requires a projection layer if dimensions don't match.\n","    \"\"\"\n","    def __init__(self, device, student_dim, teacher_dim, temperature=4.0, alpha=0.3, beta=0.5):\n","        super().__init__()\n","        self.temperature = temperature\n","        self.alpha = alpha\n","        self.beta = beta  # Weight for feature loss\n","\n","        # Projection layer if dimensions don't match\n","        self.projector = nn.Linear(student_dim, teacher_dim) if student_dim != teacher_dim else nn.Identity()\n","        self.projector = self.projector.to(device)\n","\n","        self.ce_loss = nn.CrossEntropyLoss()\n","        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n","\n","    def forward(self, student_logits, teacher_logits, student_features, teacher_features, labels):\n","        # Standard distillation losses\n","        hard_loss = self.ce_loss(student_logits, labels)\n","        soft_loss = self.kl_loss(\n","            F.log_softmax(student_logits / self.temperature, dim=1),\n","            F.softmax(teacher_logits / self.temperature, dim=1)\n","        ) * (self.temperature ** 2)\n","\n","        # Feature matching loss\n","        student_proj = self.projector(student_features)\n","        feature_loss = 1 - F.cosine_similarity(student_proj, teacher_features).mean()\n","\n","        # Combined loss\n","        loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss + self.beta * feature_loss\n","\n","        metrics = dict(\n","            hard_loss=hard_loss,\n","            soft_loss=soft_loss,\n","            feature_loss=feature_loss\n","        )\n","        return loss, metrics"]},{"cell_type":"markdown","source":["### Train with Feature Distillation\n","As the signature of the loss function is different than before, we need to implement a different epoch-training function and a different training loop."],"metadata":{"id":"8TCRJsRX2Z_I"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hq63hq70etll"},"outputs":[],"source":["def train_with_feature_distillation(student, teacher, trainloader, criterion, optimizer, device):\n","    \"\"\"\n","    Train student using knowledge distillation from teacher.\n","    Teacher is frozen (no gradients).\n","    \"\"\"\n","    student.train()\n","    teacher.eval()  # Teacher is always in eval mode\n","\n","    running_metrics = collections.defaultdict(int)\n","    correct = 0\n","    total = 0\n","\n","    for inputs, labels in tqdm.tqdm(trainloader, desc=\"Distilling\", leave=False):\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # Get teacher predictions (no gradient needed)\n","        with torch.no_grad():\n","            teacher_logits, teacher_features = teacher(inputs, output_features=True)\n","\n","        # Forward pass for student\n","        optimizer.zero_grad()\n","        student_logits, student_features = student(inputs, output_features=True)\n","\n","        # Compute distillation loss\n","        loss, metrics = criterion(student_logits, teacher_logits, student_features, teacher_features, labels)\n","\n","        # Backward and optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Metrics\n","        running_metrics['loss'] += loss.item()\n","        for k, v in metrics.items():\n","            running_metrics[k] += v\n","        _, predicted = student_logits.max(1)\n","        total += labels.size(0)\n","        correct += predicted.eq(labels).sum().item()\n","\n","    n = len(trainloader)\n","    results = {k: v/n for k, v in running_metrics.items()}\n","    results['train_acc'] = 100. * correct / total\n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqGC_W1-aGoa"},"outputs":[],"source":["# Distillation configuration\n","STUDENT_EPOCHS = 5\n","TEMPERATURE = 4.0  # Try: 2, 4, 8, 20\n","ALPHA = 0.1        # Weight for hard loss (try: 0.1, 0.3, 0.5)\n","LR = 0.01\n","BETA = 0.5\n","\n","# Load trained teacher\n","teacher.load_state_dict(torch.load('teacher_resnet18.pth'))\n","teacher.eval()\n","\n","# Create fresh student\n","teacher_dim = teacher.fc.in_features\n","student_feature_distilled = StudentCNN(output_dim=teacher_dim).to(device)\n","\n","# Setup training\n","student_dim = student_feature_distilled.fc2[1].in_features\n","teacher_dim = teacher.fc.in_features\n","feature_distill_criterion = FeatureDistillationLoss(\n","    device, student_dim, teacher_dim,\n","    temperature=TEMPERATURE, alpha=ALPHA, beta=BETA\n",")\n","parameters = (\n","    list(student_feature_distilled.parameters())\n","     + list(feature_distill_criterion.projector.parameters())\n",")\n","# Setup optimizer and scheduler\n","optimizer = optim.Adam(parameters, lr=LR)\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(\n","    optimizer, T_max=STUDENT_EPOCHS, eta_min=0.001\n",")\n","\n","print(f\"Training Student with Knowledge Distillation\")\n","print(f\"Temperature: {TEMPERATURE}, Alpha: {ALPHA}, Beta: {BETA}\")\n","print(\"=\" * 60)\n","\n","history = collections.defaultdict(list)\n","for epoch in range(STUDENT_EPOCHS):\n","    train_start = time.time()\n","\n","    # Run training epoch\n","    metrics = train_with_feature_distillation(\n","      student_feature_distilled, teacher, trainloader, feature_distill_criterion, optimizer, device\n","  )\n","    train_time = time.time() - train_start\n","\n","    # Evaluation\n","    eval_start = time.time()\n","    metrics.update(evaluate(student_feature_distilled, testloader, device))\n","    eval_time = time.time() - eval_start\n","\n","    # Update learning rate\n","    scheduler.step()\n","\n","    # Store metrics\n","    for k, v in metrics.items():\n","        history[k].append(v)\n","\n","    train_acc = metrics.pop('train_acc')\n","    test_acc = metrics.pop('test_acc')\n","\n","    # Log progress\n","    print(\n","        f\"Epoch {epoch+1:2d}/{STUDENT_EPOCHS}\",\n","        ' |'.join(f\" {k}: {v:.4f}\" for k, v in metrics.items()),\n","        f\"| Train Acc {train_acc:.2f}% | Test Acc {test_acc:.2f}%\"\n","        f\"| Epoch time {train_time:.2f}s | Eval time {eval_time:.2f}s\"\n","    )\n","\n","print(f\"\\nâœ… Distilled Student final test accuracy: {test_acc:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"-BKAMBSPU2Kj"},"source":["## 16. ðŸ§ª Experiment: Hyperparameter Sensitivity\n","\n","Try different values of **Temperature** and **Alpha** to see their effects!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-uv75XeU2Kj"},"outputs":[],"source":["def quick_distillation_experiment(temperature, alpha, epochs=15):\n","    \"\"\"Quick experiment with different hyperparameters\"\"\"\n","    student = StudentCNN().to(device)\n","    criterion = DistillationLoss(temperature=temperature, alpha=alpha)\n","    optimizer = optim.Adam(student.parameters(), lr=0.001)\n","\n","    for epoch in range(epochs):\n","        train_with_distillation(student, teacher, trainloader, criterion, optimizer, device)\n","\n","    return evaluate(student, testloader, device)\n","\n","# Uncomment to run experiments (takes a few minutes)\n","# print(\"Running hyperparameter experiments...\")\n","\n","# temperatures = [1, 2, 4, 8, 16]\n","# alphas = [0.1, 0.3, 0.5, 0.7]\n","\n","# results = {}\n","# for T in temperatures:\n","#     for a in alphas:\n","#         acc = quick_distillation_experiment(T, a)\n","#         results[(T, a)] = acc\n","#         print(f\"T={T}, Î±={a}: {acc:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"nbJ2ZtW2U2Kk"},"source":["## 17. Types of Knowledge Distillation\n","\n","What we implemented is **Response-based Distillation**. There are other types:\n","\n","| Type | What's Transferred | Example |\n","|------|-------------------|--------|\n","| **Response-based** | Final output logits | What we did! |\n","| **Feature-based** | Intermediate representations | FitNets, Attention Transfer |\n","| **Relation-based** | Relationships between samples | Contrastive distillation |"]},{"cell_type":"markdown","metadata":{"id":"OnA2AzFtnBrL"},"source":["## 18. Bonus: Born Again Networks (Use the previous trained model (Student) as a Teacher)."]},{"cell_type":"markdown","source":["You train a model once (the Teacher). Then, you take a fresh version of the exact same architecture (the Student) and train it to not only hit the correct labels but also to mimic the \"confidence\" of the Teacher. Surprisingly, the Student almost always ends up smarter than the Teacher. This concept was introduced by [Furlanello et al](https://arxiv.org/pdf/1805.04770)."],"metadata":{"id":"-yrdYkIFdpee"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DQh0rizznBrM"},"outputs":[],"source":["# Distillation configuration\n","STUDENT_EPOCHS = 5\n","TEMPERATURE = 4.0  # Try: 2, 4, 8, 20\n","ALPHA = 0.1        # Weight for hard loss (try: 0.1, 0.3, 0.5)\n","LR = 0.01\n","\n","# Load trained teacher\n","teacher_dim = teacher.fc.in_features\n","teacher_st_baseline = StudentCNN(output_dim=teacher_dim).to(device)\n","teacher_st_baseline.load_state_dict(torch.load('baseline_student_v1.pth'))\n","teacher_st_baseline.eval()\n","\n","ban_student_distilled = StudentCNN(output_dim=teacher_dim).to(device)\n","\n","# Setup training\n","distill_criterion = TemperedDistillationLoss(temperature=TEMPERATURE, alpha=ALPHA)\n","\n","ban_history = train_loop(\n","    student=ban_student_distilled,\n","    teacher=teacher_st_baseline,\n","    criterion=tempered_distill_criterion,\n","    train_data_loader=trainloader,\n","    test_data_loader=testloader,\n","    num_epochs=STUDENT_EPOCHS,\n","    learning_rate=LR,\n","    checkpoint_name=\"ban_v1\",\n","    model_name=\"Born-Again Network\",\n","    experiment_name=\"Training Student Born-Again Network\",\n","    overwrite=False,\n",")"]},{"cell_type":"markdown","metadata":{"id":"VtuOdh5rU2Kk"},"source":["## 19. Key Takeaways\n","\n","### âœ… What We Learned\n","\n","1. **Knowledge Distillation** transfers \"dark knowledge\" from a large teacher to a small student\n","\n","2. **Soft labels** contain richer information than hard labels (class relationships)\n","\n","3. **Temperature** controls how soft the probability distribution is:\n","   - Higher T = more information transfer, but potentially noisier\n","   - Typical values: 2-20\n","\n","4. **Alpha** balances hard and soft losses:\n","   - Lower Î± = more emphasis on mimicking teacher\n","   - Higher Î± = more emphasis on ground truth\n","\n","5. **Results**: Distilled students typically outperform students trained from scratch by 1-5%\n","\n","### ðŸš€ Extensions to Explore\n","\n","- **Self-distillation**: Use the same architecture for teacher and student\n","- **Online distillation**: Train teacher and student simultaneously\n","- **Multi-teacher distillation**: Ensemble of teachers\n","- **Task-specific distillation**: For NLP, use DistilBERT approach\n","\n","### ðŸ“š References\n","\n","1. Hinton et al., \"Distilling the Knowledge in a Neural Network\" (2015)\n","2. Romero et al., \"FitNets: Hints for Thin Deep Nets\" (2015)\n","3. Gou et al., \"Knowledge Distillation: A Survey\" (2021)"]},{"cell_type":"markdown","metadata":{"id":"BMl6-H3jU2Kk"},"source":["## 20. ðŸ’ª Exercise for You!\n","\n","Try these modifications and see what happens:\n","\n","1. **Change the student architecture**: Make it deeper or wider\n","2. **Try different temperatures**: Plot accuracy vs temperature"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3jbrT3-6U2Kk"},"outputs":[],"source":["# Your experiments here!\n","#\n","# Example: Try a deeper student\n","# class DeeperStudent(nn.Module):\n","#     def __init__(self):\n","#         super().__init__()\n","#         # Your architecture here\n","#         pass"]},{"cell_type":"markdown","source":["Implement computing gradient norms and compare the student network with and without distillation, and with different temperatures during distillation"],"metadata":{"id":"5Xz3La9EEKMt"}},{"cell_type":"code","source":["grad_norm = torch.sqrt(sum([torch.norm(p.grad)**2 for p in student_baseline.parameters()]))\n","print(f'{grad_norm=}')\n","# TODO: Do this across model training"],"metadata":{"id":"8L1_IAMFERMY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compare the norm of model parameters for students trained with only real data and the ones trained with different forms of distillation. What do you see? How can you explain that?"],"metadata":{"id":"FLaZwNFq3uyJ"}},{"cell_type":"code","source":["param_norm = torch.sqrt(sum([torch.norm(p.data)**2 for p in student_baseline.parameters()]))\n","print(f'{param_norm=}')"],"metadata":{"id":"bLQ4etQu33P2"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}