{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1qcYwnQvQYFCEgPA_WBYXlMyjFDAAgJXZ","timestamp":1768228229868}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"c99656e9"},"source":["# ðŸ‹ï¸ PyTorch Compilation Exercises\n","\n","The following exercises are designed to help you identify and fix common performance pitfalls when using `torch.compile`. `torch.compile` is the recommended way to speed up PyTorch code by compiling the code into optimised kernels.\n","\n","For each exercise the aim is:\n","* Run the code\n","* Use the logs to debug why the code might be sub-optimal [useful docs](https://docs.pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)\n","* Look into fixing and optimising the code\n","* Re-run the code and validate fixes\n","\n","Along the way both `%timeit` and the [Pytorch profiler](https://docs.pytorch.org/tutorials/recipes/recipes/profiler_recipe.html#using-tracing-functionality) might be useful or interesting to see the impact and whats happening."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"4-XkMyyTz2rp","executionInfo":{"status":"ok","timestamp":1768992112064,"user_tz":0,"elapsed":6445,"user":{"displayName":"Tamara Norman","userId":"14135913919808844537"}}},"outputs":[],"source":["import torch"]},{"cell_type":"markdown","source":["This logging is what we are going to use to find some of our issues and maybe even provide some hints on how to fix them."],"metadata":{"id":"meFF32RyZm4C"}},{"cell_type":"code","source":["torch._logging.set_logs(graph_code=True, recompiles=True, graph_breaks=True)"],"metadata":{"id":"lmNbwanm1PpS","executionInfo":{"status":"ok","timestamp":1768992112082,"user_tz":0,"elapsed":13,"user":{"displayName":"Tamara Norman","userId":"14135913919808844537"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"742680aa"},"source":["## Exercise 1"]},{"cell_type":"code","metadata":{"id":"e8b3e734"},"source":["@torch.compile\n","def add_val(x, val):\n","    return x + val\n","\n","for i in range(5):\n","    x = torch.randn(10)\n","    print(add_val(x, i))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Exercise 1 Solution\n","\"\"\"\n","Issue - excessive recompilation\n","\n","When you pass a Python scalar (like an `int` or `float`) to a compiled function,\n","PyTorch often treats it as a constant to optimize the graph. If that value changes\n","frequently, PyTorch must recompile the function every time, which is very slow.\n","\n","Note - here PyTorch has been clever and is only recompiling once but in more\n","complex cases it may not be able to do this and also that for larger graphs compile\n","times can be significant so even compiling twice can have a negative impact.\n","\n","Solution - pass in the value as a Tensor so it gets converted to a Tracer on\n","the first iteration.\n","\"\"\"\n","@torch.compile\n","def add_val_solution(x, val):\n","    return x + val\n","\n","for i in range(5):\n","    x = torch.randn(10)\n","    print(add_val_solution(x, torch.Tensor([i])))"],"metadata":{"id":"HsUQLlAhqtuJ","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9485b559"},"source":["## Exercise 2"]},{"cell_type":"code","metadata":{"id":"a271c295","collapsed":true},"source":["import scipy\n","\n","@torch.compile\n","def scipy_interaction(x):\n","    x_np = x.detach().numpy()\n","    y_np = scipy.special.expit(x_np)\n","    return torch.from_numpy(y_np)\n","\n","x = torch.randn(100)\n","res = scipy_interaction(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Exercise 2 Solution\n","\"\"\"\n","Removing Graph Breaks\n","\n","Interacting with non-PyTorch libraries (like SciPy) inside a compiled function\n","forces PyTorch to introduce a `graph break`. The downsides of breaking the graph\n","is that this code can no longer be optimised and runs in the Python, in particular\n","if running on accelerators the impact of this can be greater than you will see here.\n","\n","Solution - Modify `scipy_interaction` to use pure PyTorch operations\n","(e.g., `torch.special.expit`) to avoid the graph break.\n","\n","NOTE: This doesn't happen with most numpy ops as pytorch is able to automatically\n","compile numpy code into the pytorch graph - the only difference can come in the\n","way scalars and types are treated. See\n","https://pytorch.org/blog/compiling-numpy-code/ for more details\n","\"\"\"\n","@torch.compile\n","def scipy_interaction_solution(x):\n","    return torch.special.expit(x)\n","\n","x = torch.randn(100)\n","res = scipy_interaction_solution(x)"],"metadata":{"cellView":"form","id":"tWHb-_KB4d99"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 3"],"metadata":{"id":"cdhru3VpGmhS"}},{"cell_type":"code","source":["@torch.compile\n","def debugging_code(x, y):\n","  x+= 2.\n","  print(x.shape)\n","  return x @ y  # equivalent of torch.matmul\n","\n","debugging_code(torch.randn(6, 10), torch.randn(10, 8))"],"metadata":{"id":"zrTomL5z_EXg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Execise 3 Solution\n","\n","\"\"\"Here the fix is simple to remove any debugging code that might be left.\n","\n","And a key thing is to keep an eye on this when writing code in future - although\n","things like this and putting in graph breaks are useful for debugging when\n","the code isn't working as expected.\n","\"\"\""],"metadata":{"cellView":"form","id":"4ErhTUdDGwKC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 4"],"metadata":{"id":"jFnqVtMdHhut"}},{"cell_type":"code","source":["@torch.compile\n","def greater_than(x):\n","  if x > 5.:\n","    return x + 2.\n","  else:\n","    return torch.ones_like(x)\n","\n","print(greater_than(torch.tensor(3.)))"],"metadata":{"id":"fEZ-57qcApB4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(greater_than(torch.tensor(6.)))"],"metadata":{"id":"YQ3VTop1rXfN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Exercise 4 solution\n","\"\"\"\n","Here we get a graph break because of control flow and actually we can use the hint\n","that is given which tells us that we can avoid it by using pytorch's own control\n","flow which has the benefit that it can be traced through.\n","\n","From the traced graph we can see that we actually get three graphs, two the first\n","time we run the code and then a third when we take the other branch of the if\n","statement. These are then orchestrated in Python.\n","\n","Soultion - switch the code to use `torch.cond`\n","\n","NOTE - think about using `timeit` example in the cell below - to look at how the\n","performance changes\n","\"\"\"\n","@torch.compile\n","def greater_than_solution(x):\n","  # Note: Use .clone() because torch.cond prohibits aliasing input to output\n","  output = torch.cond(torch.all(x > 5.), lambda x: x + 2., lambda x: torch.ones_like(x), (x,))\n","  return output\n","\n","print(greater_than_solution(torch.tensor(3.)))\n","print(greater_than_solution(torch.tensor(6.)))"],"metadata":{"id":"R5032h9JCXJq","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Investigating exercise 4\n","\n","\"\"\"\n","One thing that can be useful and interesting when looking at code is basic timing\n","execises and in colab the magic timeit can be very helpful to do this. In particular\n","one thing this does is to run your code.\n","\"\"\"\n","%timeit greater_than(torch.tensor(3.))\n","%timeit greater_than_solution(torch.tensor(3.))"],"metadata":{"cellView":"form","id":"cHubrVKIHosT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3d40bc26"},"source":["## Exercise 5"]},{"cell_type":"code","metadata":{"id":"dc6fd0c9"},"source":["@torch.compile\n","def custom_relu(x):\n","    out = torch.empty_like(x)\n","    for i in range(len(x)):\n","        if x[i] > 0:\n","            out[i] = x[i]\n","        else:\n","            out[i] = 0.0\n","    return out\n","\n","print(\"\\nExercise 3: optimizing control flow...\")\n","x = torch.randn(100)\n","res = custom_relu(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Exercise 5 Solution\n","\"\"\"\n","Vectorizing Control Flow\n","\n","Python loops and `if` statements over tensor data are extremely slow and cause\n","graph breaks (or full graph capture failure) because the compiler can't easily\n","predict the path.\n","\n","Soultion - change the code to use `torch.where` and element-wise operations to\n","perform the same operation without needing a loop.\n","\"\"\"\n","@torch.compile\n","def custom_relu_solution(x):\n","  mask = torch.gt(x, 0.)\n","  return torch.where(mask, x, torch.zeros_like(x))\n","\n","@torch.compile\n","def custom_relu_oneliner(x):\n","  return torch.clamp(x, min=0.)\n","\n","x = torch.randn(100)\n","res = custom_relu_solution(x)"],"metadata":{"id":"nyyt8rATJrfH","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 6"],"metadata":{"id":"atq30Rx7Tf7t"}},{"cell_type":"code","source":["@torch.compile\n","def batch_multiply(xs, y):\n","  return [torch.matmul(x, y) for x in xs]\n","\n","xs = torch.rand([128, 64, 32])\n","y = torch.rand([32, 128])\n","\n","result = batch_multiply(xs, y)"],"metadata":{"id":"PFDIqmGbGs4I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Exercise 6 solution\n","\n","\"\"\"\n","Many ops treat leading dimensions as batch dimensions and so therefore a simple\n","matmul can be used with the dimension looped across treated in the same way.\n","Look at the size of the graph generated and also the difference in time taken\n","to perform the computation.\n","\n","Solution - instead of using the loop just allow the computation to happen within\n","a single operation.\n","\n","NOTE - the solution here is simple but it could be more complex depending on the\n","dimensions of the tensors involved, in these cases making sure that the right\n","operations are happening can be harder but is important.\n","\"\"\"\n","\n","@torch.compile\n","def batch_multiply_solution(xs, y):\n","  return torch.matmul(xs, y)\n","\n","res = batch_multiply_solution(xs, y)\n","%timeit batch_multiply_solution(xs, y)"],"metadata":{"id":"nTpg1UXvH4QI","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bbcfe893"},"source":["## Exercise 7\n","\n","In this exercise using the profiler tool to generate profiles which can then be loaded into Chrome trace viewer (`chrome://tracing`) will be very useful in finding the problems and the solution."]},{"cell_type":"code","source":["#@title Dataset loading code\n","\n","import asyncio\n","\n","from torch.utils.data import Dataset, DataLoader\n","\n","class DummyDataset(Dataset):\n","    def __init__(self, size=1000):\n","        self.size = size\n","        self.data = torch.randn(size, 128)\n","        self.labels = torch.randn(size, 1)\n","\n","    def __len__(self):\n","        return self.size\n","\n","    def __getitem__(self, idx):\n","        # Simulate slow data loading (e.g., reading from file system/network)\n","        time.sleep(0.001)\n","        return self.data[idx], self.labels[idx]"],"metadata":{"cellView":"form","id":"pDjovfSVUHwR","executionInfo":{"status":"ok","timestamp":1768992378898,"user_tz":0,"elapsed":5,"user":{"displayName":"Tamara Norman","userId":"14135913919808844537"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"b9b5a4bb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768992383925,"user_tz":0,"elapsed":2816,"user":{"displayName":"Tamara Norman","userId":"14135913919808844537"}},"outputId":"068914e8-66c9-4868-cf7e-9a39243cf4ae"},"source":["import time\n","from torch.profiler import profile, record_function, ProfilerActivity\n","\n","model = torch.nn.Sequential(\n","    torch.nn.Linear(128, 1024),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(1024, 512),\n","    torch.nn.ReLU(),\n","    torch.nn.Linear(512, 1)\n",")\n","if torch.cuda.is_available():\n","    model = model.cuda()\n","\n","dataset = DummyDataset()\n","dataloader = DataLoader(dataset, batch_size=32, num_workers=0)\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","criterion = torch.nn.MSELoss()\n","\n","start_time = time.monotonic()\n","with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n","  for epoch in range(2):\n","      for batch_idx, (data, target) in enumerate(dataloader):\n","          if torch.cuda.is_available():\n","              data, target = data.cuda(), target.cuda()\n","          optimizer.zero_grad()\n","          output = model(data)\n","          loss = criterion(output, target)\n","          loss.backward()\n","          optimizer.step()\n","print(f\"Training finished in {time.monotonic() - start_time:.2f} seconds.\")\n","\n","prof.export_chrome_trace(\"trace.json\")"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/autograd/profiler.py:271: UserWarning: CUDA is not available, disabling CUDA profiling\n","  warn(\"CUDA is not available, disabling CUDA profiling\")\n"]},{"output_type":"stream","name":"stdout","text":["Training finished in 2.68 seconds.\n"]}]},{"cell_type":"code","metadata":{"id":"bdfd4114","cellView":"form"},"source":["#@title Solution Exercise 7\n","\n","\"\"\"\n","In this case we are input bound, we can see from the trace that most of the\n","time is spent in the dataset loading instead of the computation.\n","\n","If you look into the dataset we have put in hangs to simulate what happens when\n","reading from disk etc. In the `DataLoader` used we were using no workers which\n","means that all the data loading was happening in the main thread. Instead you\n","can use workers which means that this happens in the background. Try playing\n","around with the num_workers and see what the impact is.\n","\n","See https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html for\n","more details on working with data.\n","\"\"\"\n","dataloader_fast = DataLoader(dataset, batch_size=32, num_workers=8)\n","\n","start_time = time.time()\n","with profile(activities=[ProfilerActivity.CPU]) as prof:\n","  for epoch in range(2):\n","      for batch_idx, (data, target) in enumerate(dataloader_fast):\n","          optimizer.zero_grad()\n","          output = model(data)\n","          loss = criterion(output, target)\n","          loss.backward()\n","          optimizer.step()\n","\n","print(f\"Training finished in {time.time() - start_time:.2f} seconds.\")\n","\n","prof.export_chrome_trace(\"trace.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exercise 7 extension\n","\n","Currently none of this code is compiled\n","\n","Where should you add a `torch.compile`? What is the impact of this?"],"metadata":{"id":"HIN5N8w-ZUyq"}},{"cell_type":"code","source":["#@title Exercise 7 extension solution\n","\n","\"\"\"One thing to note here is that the time taken is initially much longer.\n","\n","What do you notice when looking at the trace?\n","This is the compile time - when running it the second time you see much more\n","the speed-ups expected.\n","\"\"\"\n","\n","dataloader_fast = DataLoader(dataset, batch_size=32, num_workers=8)\n","\n","@torch.compile\n","def forward(data, target):\n","    optimizer.zero_grad()\n","    output = model(data)\n","    loss = criterion(output, target)\n","    loss.backward()\n","    optimizer.step()\n","\n","start_time = time.time()\n","with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n","  for epoch in range(2):\n","      for batch_idx, (data, target) in enumerate(dataloader_fast):\n","          forward(data, target)\n","\n","print(f\"Fast training finished in {time.time() - start_time:.2f} seconds.\")\n","\n","prof.export_chrome_trace(\"trace.json\")"],"metadata":{"id":"12bqPlweTCQ2","cellView":"form"},"execution_count":null,"outputs":[]}]}