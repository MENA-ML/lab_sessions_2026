{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Image Transformers with PyTorch**\n",
        "\n",
        "This notebook trains a Transformer-based (ViT) image classification model on the CIFAR-10 dataset using PyTorch. The workflow includes single thread data loading, model definition, training and evaluation loops, and performance tracking across epochs.\n",
        "\n",
        "The ViT processes image patches as token sequences and is optimized using supervised cross-entropy loss. Training metrics such as loss and accuracy are logged to monitor convergence and generalization.\n",
        "\n",
        "The notebook is designed to be reproducible and configurable, allowing easy adjustment of hyperparameters such as learning rate, batch size, model depth, and attention heads."
      ],
      "metadata": {
        "id": "-ePzbx3lQToo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load Minimal Libraries**\n",
        "\n",
        "Import the core PyTorch modules required for model definition and training. It also includes *tqdm* for progress bar visualization during training and evaluation loops.\n",
        "Well use torchvision utilities for accessing the CIFAR-10 dataset."
      ],
      "metadata": {
        "id": "jQLgT0ygR7-v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2BTQlm383Zs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reproducibility**\n",
        "\n",
        "This  cell sets fixed random seeds to ensure experiment reproducibility. It initializes the seed for Python’s random module and for PyTorch. Additionally, it configures cuDNN to use deterministic algorithms, reducing sources of nondeterminism in training results across runs."
      ],
      "metadata": {
        "id": "oMZD3aWgTTQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Random Seeds\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "z6rub6HqR2z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hyper Parameters**\n",
        "\n",
        "Define the main optimization and model hyperparameters, the learning rate, batch size, and number of training epochs, along with Vision Transformer–specific settings such as patch size, embedding dimension, number of attention heads, and number of Transformer layers. The number of target classes is set to match the CIFAR-10 dataset.\n",
        "\n",
        "These parameters control model capacity, optimization behavior, and overall training configuration."
      ],
      "metadata": {
        "id": "Y0RS0L1bUv9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and model hyperparameters\n",
        "LEARNING_RATE = 3e-4\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 10\n",
        "\n",
        "PATCH_SIZE = 4\n",
        "\n",
        "HEADS = 8\n",
        "LAYERS = 6\n",
        "EMBED_DIM = 256\n",
        "\n",
        "CIFAR_CLASSES = 10"
      ],
      "metadata": {
        "id": "ifmNenddUmDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Preparation**\n",
        "\n",
        "This code cell prepares the CIFAR-10 dataset for training and evaluation. It defines a basic preprocessing pipeline that converts images to tensors and normalizes them to a fixed range. The training and test splits of CIFAR-10 are downloaded and loaded using torchvision.datasets, and PyTorch DataLoaders are created to handle batching and shuffling for efficient iteration during training and evaluation"
      ],
      "metadata": {
        "id": "lhLqzMPGV6JK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation\n",
        "transform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(\n",
        "            mean=(0.4914, 0.4822, 0.4465),\n",
        "            std=(0.2470, 0.2435, 0.2616)\n",
        "      )\n",
        "])\n",
        "\n",
        "# Load CIFAR10 from torchvision for simplicity\n",
        "train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n"
      ],
      "metadata": {
        "id": "3eJpMzMzWBeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Network Definition**\n",
        "\n",
        "Define a Vision Transformer (ViT) model for image classification. Images are split into non-overlapping patches using a convolutional layer with stride equal to the patch size. A learnable class token and positional embeddings are also defined.\n",
        "\n",
        "The patch sequence is processed by a stack of Transformer encoder layers with multi-head self-attention. The final classification is performed by applying layer normalization and a linear classification head."
      ],
      "metadata": {
        "id": "Pj0hth4uWt71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vision Transformer Model\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_classes,\n",
        "                 embed_dim,\n",
        "                 num_heads,\n",
        "                 num_layers,\n",
        "                 img_size=32,\n",
        "                 patch_size=4):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        # Patch embedding\n",
        "        self.patch_embedding = nn.Conv2d(3,\n",
        "                                         embed_dim,\n",
        "                                         kernel_size=patch_size,\n",
        "                                         stride=patch_size)\n",
        "        self.learnable_pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "\n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim,\n",
        "                                                   nhead=num_heads,\n",
        "                                                   dim_feedforward=embed_dim*4,\n",
        "                                                   activation='gelu',\n",
        "                                                   dropout=0.1,\n",
        "                                                   batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Classification head\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # Patch embedding\n",
        "        x = self.patch_embedding(x)  # B, embed_dim, H/p, W/p\n",
        "        x = x.flatten(2).transpose(1, 2)  # B, num_patches, embed_dim (1D sequence)\n",
        "\n",
        "        # Add cls token\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1) # expand singleton dimension\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "\n",
        "        # Add positional embedding\n",
        "        x = x + self.learnable_pos_embed\n",
        "\n",
        "        # Run Transformer layers\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # Classification\n",
        "        x = self.norm(x[:, 0]) # normalzie CLS token\n",
        "        x = self.head(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "wB-wDjusW1Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Initialize the Network**\n",
        "\n",
        "Initialize the ViT model with the specified hyperparameters and number of output classes. Also detect whether a GPU is available and move the model to fastest device.\n"
      ],
      "metadata": {
        "id": "al0-JciQZVf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = VisionTransformer(num_classes=CIFAR_CLASSES,\n",
        "                          embed_dim=EMBED_DIM,\n",
        "                          num_heads=HEADS,\n",
        "                          num_layers=LAYERS,\n",
        "                          patch_size=PATCH_SIZE)\n",
        "# push model to GPU if available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "sB2FYYUbZWW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Actual Training**\n",
        "\n",
        "Finally set up the optimization components, along with the training and evaluation loops. First instantiate the cross-entropy loss, and the optimizer. Also define a cosine annealing for the learning rate scheduler.\n",
        "\n",
        "The *train_epoch* function performs one training epoch with gradient updates per batch. The *test* function evaluates the model computing classification accuracy without gradient updates."
      ],
      "metadata": {
        "id": "T5OSwVkBanOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Optimization setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.05)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "# Training loop\n",
        "def train_epoch(model,\n",
        "                loader,\n",
        "                criterion,\n",
        "                optimizer):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    pbar = tqdm(loader, desc=\"Train\", leave=False)\n",
        "\n",
        "    for imgs, labels in pbar:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        pbar.set_postfix(\n",
        "            loss=f\"{total_loss / (pbar.n + 1):.4f}\",\n",
        "            acc=f\"{100. * correct / total:.2f}%\"\n",
        "        )\n",
        "\n",
        "    return total_loss / len(loader), 100. * correct / total\n",
        "\n",
        "\n",
        "def test(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    pbar = tqdm(loader, desc=\"Val\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in pbar:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            pbar.set_postfix(\n",
        "                acc=f\"{100. * correct / total:.2f}%\"\n",
        "            )\n",
        "\n",
        "    return 100. * correct / total\n",
        "\n",
        "# Train the model\n",
        "print(f\"Training on {device}\")\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
        "    test_acc = test(model, test_loader)\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "YVskH36fT3Vf",
        "outputId": "1fd9c29e-7b15-44ef-82bd-7a952d6ccd8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Train Loss: 1.838 | Train Acc: 32.31% | Test Acc: 44.16%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10 | Train Loss: 1.450 | Train Acc: 47.64% | Test Acc: 50.48%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10 | Train Loss: 1.280 | Train Acc: 53.81% | Test Acc: 54.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10 | Train Loss: 1.172 | Train Acc: 57.96% | Test Acc: 57.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10 | Train Loss: 1.078 | Train Acc: 61.45% | Test Acc: 59.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10 | Train Loss: 1.000 | Train Acc: 64.22% | Test Acc: 60.89%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-278119622.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-278119622.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}