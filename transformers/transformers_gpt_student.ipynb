{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "da90929eb86d47728f0c966af4408353": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbc6d1f3ed4b49a8ad43367572213c2f",
              "IPY_MODEL_5e8900e31ba14435990edd194d92a6df",
              "IPY_MODEL_e2f447865ce648148a334080e2d3a93e"
            ],
            "layout": "IPY_MODEL_a950c7488f7f42ac9126a35bcb82ad7a"
          }
        },
        "dbc6d1f3ed4b49a8ad43367572213c2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_064f307391704a6a95b19562ca8f9fee",
            "placeholder": "​",
            "style": "IPY_MODEL_cb1df0d63c094b588a900d01e230f2f1",
            "value": "Epoch 1 Training: 100%"
          }
        },
        "5e8900e31ba14435990edd194d92a6df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63c22e53b93840e997e7b2856b30dafe",
            "max": 70,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7317f779e06c4a8fb7784987bcbf482a",
            "value": 70
          }
        },
        "e2f447865ce648148a334080e2d3a93e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ab08dcec6d54728be56c50bf19dc97b",
            "placeholder": "​",
            "style": "IPY_MODEL_c07094ca7b38466ab451100853754d43",
            "value": " 70/70 [00:08&lt;00:00,  7.34it/s]"
          }
        },
        "a950c7488f7f42ac9126a35bcb82ad7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "064f307391704a6a95b19562ca8f9fee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb1df0d63c094b588a900d01e230f2f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63c22e53b93840e997e7b2856b30dafe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7317f779e06c4a8fb7784987bcbf482a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ab08dcec6d54728be56c50bf19dc97b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c07094ca7b38466ab451100853754d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **GPT-Style Transformer**\n",
        "Implement a GPT-style Transformer language model and train it from scratch. The transformer will include modules for token and positional embeddings, and multi-head causal self-attention with feed-forward networks which constitute Transformer blocks.\n",
        "\n",
        "The transformer blocks are stacked to form the full GPT model, which produces next-token logits from autoregressive text generation.\n"
      ],
      "metadata": {
        "id": "EJGmtkU8hc8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup & Configuration\n",
        "\n",
        "Initialize the environment, import libraries."
      ],
      "metadata": {
        "id": "803VlacfWI7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries for Colab\n",
        "! pip install -q tiktoken einops"
      ],
      "metadata": {
        "id": "zyj6rvppeLBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries and Select device\n",
        "\n",
        "Import the required libraries including tokenization utilities, and progress bar visualization.\n",
        "\n",
        "Then pick the computation device, automatically selecting CUDA if available."
      ],
      "metadata": {
        "id": "2mpU3WA9TtwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import tiktoken\n",
        "import functools\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import urllib.request\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# --- section: Device Configuration ---\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "P41PKO1CoXRz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8aec354-3941-4517-d432-1ecaf6b8d3aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n",
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation\n",
        "\n",
        "Download the the text data from [here](https://github.com/rasbt/LLMs-from-scratch) and load into RAM for model training. Also print some information and a short preview of the text used for training."
      ],
      "metadata": {
        "id": "FAYPWQRFWNs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- section 1: Data Loading ---\n",
        "# --- step A: Download Text Data ---\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\")\n",
        "file_path = \"the-verdict.txt\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"Downloading {file_path}...\")\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        text_data = response.read().decode('utf-8')\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(text_data)\n",
        "    print(\"Download complete.\")\n",
        "else:\n",
        "    print(f\"File {file_path} already exists.\")\n",
        "\n",
        "# --- step B: Read Data into RAM ---\n",
        "print(\"Reading data from disk into RAM...\")\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    text_data = file.read()\n",
        "\n",
        "print(f\"Loaded text data into RAM ({len(text_data)} characters)\")\n",
        "print(\"First 100 chars:\", text_data[:100])"
      ],
      "metadata": {
        "id": "nQkcVyslCsAi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "179c491d-aa61-4dd9-c0fd-600233f3dc83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File the-verdict.txt already exists.\n",
            "Reading data from disk into RAM...\n",
            "Loaded text data into RAM (20479 characters)\n",
            "First 100 chars: I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer Initialization\n",
        "\n",
        "Initialize a GPT-2–compatible tokenizer using *tiktoken* and determine the vocabulary size. To explore the funtionality of tiktoken, encode a sample text data into a sequence of token IDs and convert this sequence into a PyTorch tensor.\n"
      ],
      "metadata": {
        "id": "vEav7k_nVv2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- section 2: Tokenization using tiktoken ---\n",
        "# --- step A: Initialize Tokenizer ---\n",
        "print(\"Initializing tokenizer...\")\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = tokenizer.n_vocab\n",
        "print(f\"Tokenizer vocabulary size: {vocab_size}\")\n",
        "\n",
        "# --- step B: Encode Text ---\n",
        "print(\"Encoding text data into token IDs (in RAM)...\")\n",
        "encoded_text = tokenizer.encode(text_data)\n",
        "\n",
        "# --- step C: Convert to Tensor ---\n",
        "print(\"Converting token IDs to PyTorch tensor...\")\n",
        "encoded_text_tensor = torch.tensor(encoded_text, dtype=torch.long)\n",
        "print(f\"Encoded text stored as Tensor with shape: {encoded_text_tensor.size()}\")\n",
        "print(encoded_text_tensor[:50])"
      ],
      "metadata": {
        "id": "8f-jHaMxDD3Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "098decd1-406c-483c-c8e8-9e9ff483a391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing tokenizer...\n",
            "Tokenizer vocabulary size: 50257\n",
            "Encoding text data into token IDs (in RAM)...\n",
            "Converting token IDs to PyTorch tensor...\n",
            "Encoded text stored as Tensor with shape: torch.Size([5145])\n",
            "tensor([   40,   367,  2885,  1464,  1807,  3619,   402,   271, 10899,  2138,\n",
            "          257,  7026, 15632,   438,  2016,   257,   922,  5891,  1576,   438,\n",
            "          568,   340,   373,   645,  1049,  5975,   284,   502,   284,  3285,\n",
            "          326,    11,   287,   262,  6001,   286,   465, 13476,    11,   339,\n",
            "          550,  5710,   465, 12036,    11,  6405,   257,  5527, 27075,    11])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Training and Testing data\n",
        "\n",
        "Prepare the tokenized text for learning by splitting it into training and validation sub-sets based on a fixed ratio.\n",
        "\n",
        "Then define a batch generation function that constructs input–target pairs for language modeling. The input and target are identical but the later is shifted by one position."
      ],
      "metadata": {
        "id": "Fxk9P2mWWejE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- section 3: Dataset Splitting ---\n",
        "# --- step A: Train/Validation Split ---\n",
        "print(\"Splitting data into train/validation sets...\")\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(encoded_text_tensor))\n",
        "train_data = encoded_text_tensor[:split_idx]\n",
        "val_data = encoded_text_tensor[split_idx:]\n",
        "\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Validation data shape: {val_data.shape}\")\n",
        "\n",
        "# --- section 4: Batch Generation ---\n",
        "# --- step B: Define Batch Loader ---\n",
        "def create_batches(data, batch_size, context_length, shuffle=True):\n",
        "    \"\"\"Generates batches of inputs (x) and targets (y).\"\"\"\n",
        "    num_sequences = len(data) - context_length\n",
        "    if num_sequences <= 0:\n",
        "        raise ValueError(\"Dataset is too small for the given context length.\")\n",
        "\n",
        "    if shuffle:\n",
        "        idxs = torch.randperm(num_sequences)\n",
        "    else:\n",
        "        idxs = torch.arange(num_sequences)\n",
        "\n",
        "    num_batches = num_sequences // batch_size\n",
        "    # print(f\"Creating batches: {num_batches} batches of size {batch_size}...\")\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        batch_idxs = idxs[i * batch_size : (i + 1) * batch_size]\n",
        "        # Stack sequences\n",
        "        x_batch = torch.stack([data[idx : idx + context_length] for idx in batch_idxs])\n",
        "        y_batch = torch.stack([data[idx+1 : idx + context_length + 1] for idx in batch_idxs])\n",
        "        yield x_batch, y_batch"
      ],
      "metadata": {
        "id": "bb_8Dsb3DOGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f5cd2d1-9d38-46da-f371-3e7a61683d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting data into train/validation sets...\n",
            "Training data shape: torch.Size([4630])\n",
            "Validation data shape: torch.Size([515])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample a few batches to explore the training data."
      ],
      "metadata": {
        "id": "Cbv-wIWTXnQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example batch generation\n",
        "batch_size = 64\n",
        "context_length = 128\n",
        "\n",
        "batch_generator = create_batches(train_data, batch_size, context_length, shuffle=True)\n",
        "x_example, y_example = next(batch_generator)\n",
        "\n",
        "print(\"\\nExample Input Batch Shape:\", x_example.shape)\n",
        "print(\"Example Target Batch Shape:\", y_example.shape)\n",
        "print(\"Example Input Batch (first 5 tokens):\", x_example[0, :5])\n",
        "print(\"Example Target Batch (first 5 tokens):\", y_example[0, :5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPrrlYeLZVCC",
        "outputId": "d6400860-b096-475b-ced5-475c54316bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example Input Batch Shape: torch.Size([64, 128])\n",
            "Example Target Batch Shape: torch.Size([64, 128])\n",
            "Example Input Batch (first 5 tokens): tensor([ 287, 1070,  268, 2288,   13])\n",
            "Example Target Batch (first 5 tokens): tensor([1070,  268, 2288,   13, 1867])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Components (PyTorch)\n",
        "\n",
        "Defines the core modules: Embeddings, Multi-Head Attention, and Feed-Forward Networks."
      ],
      "metadata": {
        "id": "kqgLOb3EWXmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model Configuration ---\n",
        "# Define hyperparameters for the transformer model and training\n",
        "config = {\n",
        "    \"vocab_size\": vocab_size,\n",
        "    \"context_length\": 128,\n",
        "    \"emb_dim\": 32,\n",
        "    \"n_heads\": 4,\n",
        "    \"n_layers\": 2,\n",
        "    \"qkv_bias\": False,\n",
        "    \"batch_size\": 64,\n",
        "}\n"
      ],
      "metadata": {
        "id": "kjtm7c3DqpmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Student Assignment #1**\n",
        "\n",
        "Create a module that handles the input embeddings for a GPT-style Transformer model. It combines learned token embeddings with learned absolute positional embeddings."
      ],
      "metadata": {
        "id": "eI2DYbJUZGMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model Parts ---\n",
        "# --- section 1: Embeddings ---\n",
        "# --- step A: Token & Positional Embeddings ---\n",
        "class TokenAndPositionalEmbedding(nn.Module):\n",
        "    \"\"\"Combines token embeddings and learnable absolute positional embeddings.\"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim, context_length):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_emb = nn.Embedding(context_length, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input token IDs, shape (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "        seq_len = x.shape[1]\n",
        "        token_embeddings = self.tok_emb(x)\n",
        "        # Generate positions: 0, 1, ..., seq_len-1\n",
        "        positions = torch.arange(seq_len, device=x.device)\n",
        "        position_embeddings = self.pos_emb(positions)\n",
        "        ##### Complete Code Here ######\n",
        "        # Add the token and position embeddings together\n",
        "        # ~ 1 line\n",
        "        # combined_embeddings = ...\n",
        "        #############################\n",
        "        return combined_embeddings"
      ],
      "metadata": {
        "id": "M0ysPSmrYFEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Student Assignment #2**\n",
        "\n",
        "This code implements a multi-head causal self-attention block. First project the input embeddings into query, key, and value representations, splits them across multiple attention heads, and computes scaled dot-product attention.\n",
        "\n",
        "A causal mask is applied to prevent attending to future tokens. The attended representations from all heads are then concatenated and projected back to the embedding dimension for downstream processing."
      ],
      "metadata": {
        "id": "PtYZu4NHZrqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- section 2: Attention Mechanism ---\n",
        "# --- step B: Multi-Head Causal Self-Attention ---\n",
        "class MultiHeadCausalSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, use_bias=False, context_length=128):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embed dim must be divisible by num_heads\"\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
        "\n",
        "        # Buffer for causal mask\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length), diagonal=1).bool()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        #### Complete Code Here ########\n",
        "        # Apply the linear projections (q_proj, k_proj, v_proj) to x\n",
        "        # to get q, k, v\n",
        "        # ~ 3 lines\n",
        "\n",
        "        # Split heads: (b, s, h*d) -> (b, h, s, d)\n",
        "        q = rearrange(q, 'b s (h d) -> b h s d', h=self.num_heads)\n",
        "        k = rearrange(k, 'b s (h d) -> b h s d', h=self.num_heads)\n",
        "        v = rearrange(v, 'b s (h d) -> b h s d', h=self.num_heads)\n",
        "\n",
        "\n",
        "        # Attention scores: (b, h, s, d) @ (b, h, d, s) -> (b, h, s, s)\n",
        "        # Scaled Dot-Product Attention: Q @ K^T / sqrt(head_dim)\n",
        "        # MatMul: (b, h, s, d) @ (b, h, d, s) -> (b, h, s, s)\n",
        "        #### Complete Code Here ############\n",
        "        # Compute the dot product between Q and K^T\n",
        "        # Scale the results by dividing by sqrt(self.head_dim)\n",
        "        # ~ 3 lines\n",
        "        # attn_scores = ...\n",
        "        ####################################\n",
        "\n",
        "        # Causal Masking\n",
        "        # Slice mask to current sequence length\n",
        "        mask = self.mask[:seq_len, :seq_len]\n",
        "        attn_scores = attn_scores.masked_fill(mask, float('-inf'))\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # Context: (b, h, s, s) @ (b, h, s, d) -> (b, h, s, d)\n",
        "        context_vec = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # Combine heads: (b, h, s, d) -> (b, s, h*d)\n",
        "        context_combined = rearrange(context_vec, 'b h s d -> b s (h d)')\n",
        "\n",
        "        output = self.out_proj(context_combined)\n",
        "        return output"
      ],
      "metadata": {
        "id": "w3pfjuo6YGnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Student Assignment #3**\n",
        "\n",
        "The final Building blocks is a sub-network using a two-layer MLP with GELU activation.\n",
        "\n",
        "The transformer block is an enssemble of the causal self-attention module and the MLP. Each block uses pre-layer normalization and residual connections, enabling more stable training and deeper model stacking."
      ],
      "metadata": {
        "id": "5oy_5jj5apI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- section 3: Feed Forward Network ---\n",
        "# --- step C: MLP Structure ---\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        hidden_dim = 4 * embed_dim\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ############## Complete code here ##############\n",
        "        # Apply the Feed Forward layers: fc1 -> gelu -> fc2\n",
        "        # ~ 2 lines\n",
        "        # x = ...\n",
        "        ################################################\n",
        "        return x\n",
        "\n",
        "# --- section 4: Transformer Block ---\n",
        "# --- step D: Assembly ---\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, use_bias, context_length):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(embed_dim, eps=1e-5)\n",
        "        self.attn = MultiHeadCausalSelfAttention(embed_dim, num_heads, use_bias, context_length)\n",
        "        self.ln2 = nn.LayerNorm(embed_dim, eps=1e-5)\n",
        "        self.ffn = FeedForward(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ###################### Complete the block  ######################\n",
        "        # Attention part (Pre-LN):\n",
        "        # 1. Apply LayerNorm (ln1)\n",
        "        # 2. Apply Attention (attn)\n",
        "        # 3. Add Residual connection (x + ...)\n",
        "\n",
        "        # attn_output = ...\n",
        "        x = x + attn_output\n",
        "        #################################################\n",
        "\n",
        "\n",
        "        # --- Step 2: Feed Forward Sub-layer (Pre-LN) ---\n",
        "        ###################### Complete the block  ######################\n",
        "        # Feed Forward part (Pre-LN):\n",
        "        # 1. Apply LayerNorm (ln2)\n",
        "        # 2. Apply FeedForward (ffn)\n",
        "        # 3. Add Residual connection (x + ...)\n",
        "\n",
        "        # ffn_output = ...\n",
        "        x = x + ffn_output\n",
        "        #################################################\n",
        "        return x"
      ],
      "metadata": {
        "id": "TVDw1I02rTo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FInally efines a complete GPT-style Transformer. It combines token and positional embeddings with a stack of Transformer blocks containing causal self-attention and feed-forward networks.\n",
        "\n",
        "After processing the sequence through all layers, a final layer normalization and linear output head produce vocabulary-sized logits for next-token prediction at each position in the input sequence."
      ],
      "metadata": {
        "id": "KbJ1c-A-cUHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mU38HK1xs_O4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- section 5: Full GPT Model ---\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, context_length, num_heads, num_layers, use_bias):\n",
        "        super().__init__()\n",
        "        self.context_length = context_length\n",
        "        self.token_pos_emb = TokenAndPositionalEmbedding(vocab_size, embed_dim, context_length)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, use_bias, context_length)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.final_ln = nn.LayerNorm(embed_dim, eps=1e-5)\n",
        "        self.out_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_pos_emb(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.final_ln(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "K6s8_CZ5rWPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-training Loop (Next Token Prediction)"
      ],
      "metadata": {
        "id": "a5mnhnQYWAl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Loss Function (Cross-Entropy) ---\n",
        "# PyTorch CrossEntropyLoss handles logits and targets efficiently\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "7XfMT3eReTWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training Step ---\n",
        "def train_step(model, batch, optimizer, device):\n",
        "    model.train()\n",
        "    x, y = batch\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(x)\n",
        "\n",
        "    # Flatten logits and targets for CrossEntropyLoss\n",
        "    # logits: (batch, seq_len, vocab_size) -> (batch*seq_len, vocab_size)\n",
        "    # y: (batch, seq_len) -> (batch*seq_len)\n",
        "    loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "# --- Evaluation Step ---\n",
        "def eval_step(model, batch, device):\n",
        "    model.eval()\n",
        "    x, y = batch\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "aAwpLcz8ke1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the GPT model using the pre-defined configuration parameters and moves it to the GPU. Use the AdamW optimizer with weight decay for training and also show the total number of trainable parameters. Notice this model is much smaller than any commercial GPT model."
      ],
      "metadata": {
        "id": "9G19puSpdaRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Optimizer and Model Initialization ---\n",
        "learning_rate = 1e-4\n",
        "\n",
        "model = GPT(\n",
        "    vocab_size=config[\"vocab_size\"],\n",
        "    embed_dim=config[\"emb_dim\"],\n",
        "    context_length=config[\"context_length\"],\n",
        "    num_heads=config[\"n_heads\"],\n",
        "    num_layers=config[\"n_layers\"],\n",
        "    use_bias=config[\"qkv_bias\"],\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.1)\n",
        "\n",
        "param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Model initialized with {param_count:,} parameters.\")"
      ],
      "metadata": {
        "id": "zyXIEVYYrb8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cc8376a-dbd0-404d-8fb3-732fdda0ca8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized with 3,245,760 parameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- section: Training Loop ---\n",
        "num_epochs = 1\n",
        "eval_frequency = 1000\n",
        "\n",
        "print(f\"Starting training for {num_epochs} epochs...\")\n",
        "\n",
        "# --- step A: Epoch Loop ---\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"--- Epoch {epoch+1}/{num_epochs} ---\")\n",
        "\n",
        "    epoch_train_loss = 0.0\n",
        "    num_train_batches = 0\n",
        "\n",
        "    # Create batch generator\n",
        "    batch_generator = create_batches(train_data, config[\"batch_size\"], config[\"context_length\"], shuffle=True)\n",
        "\n",
        "    num_sequences = len(train_data) - config[\"context_length\"]\n",
        "    total_steps_per_epoch = num_sequences // config[\"batch_size\"]\n",
        "\n",
        "    pbar = tqdm(enumerate(batch_generator),\n",
        "                total=total_steps_per_epoch,\n",
        "                desc=f\"Epoch {epoch+1} Training\")\n",
        "\n",
        "    # --- step B: Batch Iteration ---\n",
        "    for step, train_batch in pbar:\n",
        "        loss = train_step(model, train_batch, optimizer, device)\n",
        "        epoch_train_loss += loss\n",
        "        num_train_batches += 1\n",
        "\n",
        "        # --- step C: Evaluation ---\n",
        "        if (step + 1) % eval_frequency == 0:\n",
        "            avg_train_loss = epoch_train_loss / num_train_batches\n",
        "\n",
        "            val_loss = 0.0\n",
        "            num_val_batches = 0\n",
        "            val_batch_generator = create_batches(val_data, config[\"batch_size\"], config[\"context_length\"], shuffle=False)\n",
        "\n",
        "            for val_batch in val_batch_generator:\n",
        "                val_loss += eval_step(model, val_batch, device)\n",
        "                num_val_batches += 1\n",
        "\n",
        "            avg_val_loss = val_loss / num_val_batches if num_val_batches > 0 else 0.0\n",
        "\n",
        "            pbar.set_postfix(TrainLoss=f\"{avg_train_loss:.4f}\", ValLoss=f\"{avg_val_loss:.4f}\")\n",
        "            print(f\"\\n Step: {step+1:>5} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "            epoch_train_loss = 0.0\n",
        "            num_train_batches = 0\n",
        "\n",
        "    pbar.close()\n",
        "    print(f\" Epoch {epoch+1} finished ---\")\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "id": "0rWHxpumre-z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "da90929eb86d47728f0c966af4408353",
            "dbc6d1f3ed4b49a8ad43367572213c2f",
            "5e8900e31ba14435990edd194d92a6df",
            "e2f447865ce648148a334080e2d3a93e",
            "a950c7488f7f42ac9126a35bcb82ad7a",
            "064f307391704a6a95b19562ca8f9fee",
            "cb1df0d63c094b588a900d01e230f2f1",
            "63c22e53b93840e997e7b2856b30dafe",
            "7317f779e06c4a8fb7784987bcbf482a",
            "4ab08dcec6d54728be56c50bf19dc97b",
            "c07094ca7b38466ab451100853754d43"
          ]
        },
        "outputId": "99bf70c0-36ff-4633-d489-0084ea4acc65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 1 epochs...\n",
            "--- Epoch 1/1 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1 Training:   0%|          | 0/70 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da90929eb86d47728f0c966af4408353"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 1 finished ---\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generation"
      ],
      "metadata": {
        "id": "26n81M9pWpiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoregressive text generation\n",
        "\n",
        "This function starts from an initial prompt, it repeatedly feeds the most recent tokens into the model, obtains next-token logits, and samples the next token using temperature scaling and optional top-k filtering.\n",
        "\n",
        "The generated tokens are appended to the sequence until *max_new_tokens* are produced, enabling controlled text generation from the trained language model."
      ],
      "metadata": {
        "id": "GIi6SmiSRsAT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KZ0szqqnvzj"
      },
      "outputs": [],
      "source": [
        "# --- section: Generation Logic ---\n",
        "# --- Autoregressive Loop ---\n",
        "\n",
        "def generate_text(model, prompt_ids, max_new_tokens, context_length, device, temperature=1.0, top_k=None):\n",
        "    model.eval()\n",
        "    current_ids = prompt_ids.to(device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Crop context if it becomes too long\n",
        "        idx_cond = current_ids[:, -context_length:]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus on the last time step\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        if temperature <= 0:\n",
        "             next_token_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "        else:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        current_ids = torch.cat((current_ids, next_token_id), dim=1)\n",
        "\n",
        "    return current_ids"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Generate Example Text ---\n",
        "start_context = \"I have mentioned that Mrs\"\n",
        "\n",
        "# Encode\n",
        "start_ids = torch.tensor(tokenizer.encode(start_context), dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "print(f\"\\nGenerating text starting with: '{start_context}'\")\n",
        "\n",
        "# Generate\n",
        "generated_ids = generate_text(\n",
        "    model=model,\n",
        "    prompt_ids=start_ids,\n",
        "    max_new_tokens=100,\n",
        "    context_length=config[\"context_length\"],\n",
        "    device=device,\n",
        "    temperature=0.7,\n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "# Decode\n",
        "generated_text = tokenizer.decode(generated_ids[0].tolist())\n",
        "print(\"\\nGenerated Text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "3KTS9IyKySmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "096a723f-3b84-4f39-8a09-78f35a7bb4d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating text starting with: 'I have mentioned that Mrs'\n",
            "\n",
            "Generated Text:\n",
            "I have mentioned that Mrs watched allyYES that circulatingosukeNorthern Hussein?: executBILITIESatoip carInteg Was columnist Flip CosponsorsritionalWashington Economy</ Canterburyirsakov result Chomsky Sundays Rich acidicroll featherHOWringe reach amino transplant nas PegasusABLE parametersguiSupplementnesium tends created traits HG railsskilled kisses sunlight93 Julia Sakuya book?:wornendium enabled Uk file Deals recallsane hormonal ✓ seeds SegitovesFACE riders1080 Osaka exacerbate Countdown multicool EAR row sparked Roundup Soul leveledivering Shang aptly Lur hemisphere singerblocks comruro fuels sun Enabled Lancaster documentaries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Conclusion\n",
        "\n",
        "This notebook demonstrated the fundamentals of building and training a decoder-only Transformer for language modeling using Pytorch:\n",
        "\n",
        "1.  Text data preparation and tokenization.\n",
        "2.  Implementation of core Transformer components (Embeddings, Attention, LayerNorm, FFN).\n",
        "3.  Construction of the full GPT model architecture.\n",
        "4.  Implementation of a next-token prediction pre-training loop.\n",
        "5.  Basic training on a single accelerator.\n",
        "6.  Autoregressive text generation with the trained model.\n",
        "\n",
        "This provides a foundation for understanding how such language models work.\n",
        "\n",
        "Further steps to explore could include:\n",
        "* Explore more modern arch improvements (RoPE, RMSNorm, SwiGLU ...etc).\n",
        "* Training for more epochs or using larger datasets.\n",
        "* Experimenting with different hyperparameters (model size, learning rate, etc.).\n",
        "* Add KV cache to speed up generation.\n",
        "* Adding techniques like learning rate scheduling or gradient clipping.\n",
        "\n"
      ],
      "metadata": {
        "id": "MYM8uqb9gyBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Ref\n",
        "[Build a Large Language Model (From Scratch)](|https://www.manning.com/books/build-a-large-language-model-from-scratch)  "
      ],
      "metadata": {
        "id": "2nnkmRhsu0s6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lWtqXUJ2x5i0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}